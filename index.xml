<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>HPC on IBM Cloud Tutorials</title><link>https://ibm.github.io/ibmcloud-hpc-tutorials/</link><description>Recent content on HPC on IBM Cloud Tutorials</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Wed, 18 Sep 2019 10:50:17 -0400</lastBuildDate><atom:link href="https://ibm.github.io/ibmcloud-hpc-tutorials/index.xml" rel="self" type="application/rss+xml"/><item><title>Create VSI From Default LSF Image</title><link>https://ibm.github.io/ibmcloud-hpc-tutorials/04-custom-image/01-create-vsi-from-default-lsf-image/</link><pubDate>Mon, 24 Jan 2022 15:54:40 -0500</pubDate><guid>https://ibm.github.io/ibmcloud-hpc-tutorials/04-custom-image/01-create-vsi-from-default-lsf-image/</guid><description>Start IBM Cloud Shell from the browser Find the icon to lauch Cloud Shell in the dash board:
It will create another browser window with the Cloud Shell:
In the Cloud Shell, follow the next steps to create a VSI (assuming bash):
Assign values to environment variables for commands later # Replace CLUSTER_PREFIX with the one you used to create the existing LSF cluster # Replace KEY_NAME, RESOURCE_GROUP_NAME, ZONE_NAME and CLUSTER_IMG_ID as needed CLUSTER_PREFIX=&amp;lt;cluster-name-prefix&amp;gt; CLUSTER_VPC_NAME=${CLUSTER_PREFIX}-vpc CLUSTER_SUBNET_NAME=${CLUSTER_PREFIX}-subnet CLUSTER_IMG_ID=r006-68478a2e-4abc-4bfb-9e4f-a6fb3b9b235f CLUSTER_SG_NAME=${CLUSTER_PREFIX}-sg CUSTOM_IMG_NAME=${CLUSTER_PREFIX}-custom-img CUSTOM_IMG_VSI_NAME=${CUSTOM_IMG_NAME}-vsi ZONE_NAME=us-south-3 PROFILE_NAME=bx2-2x8 KEY_NAME=ccyang-pub-key RESOURCE_GROUP_NAME=Default The CLUSTER_IMG_ID can be found from the open-source terraform script in IBM Cloud hpc-cluster-lsf github repository.</description></item><item><title>Emulate an on-premise cluster</title><link>https://ibm.github.io/ibmcloud-hpc-tutorials/03-bursting/01-onprem_cluster/</link><pubDate>Sun, 02 Jan 2022 01:00:00 -0100</pubDate><guid>https://ibm.github.io/ibmcloud-hpc-tutorials/03-bursting/01-onprem_cluster/</guid><description>The first step is to create an LSF cluster in IBM Cloud US-South (Dallas) region, which is to be used as an emulation of a &amp;ldquo;on-premise&amp;rdquo; cluster.
If you have not created an LSF cluster in us-south, you can repeat the steps in Standing up and LSF Cluster. For simplicity, please use the following setup:
management_node_count=1 worker_node_min_count=2 worker_node_max_count=2 Remember, if you want to start over, always do a Destroy resources from the Workspace interface first, to make sure no zombie resources are left behind.</description></item><item><title>Customize Software on the VSI</title><link>https://ibm.github.io/ibmcloud-hpc-tutorials/04-custom-image/02-customize-software-on-the-vsi/</link><pubDate>Mon, 24 Jan 2022 15:55:41 -0500</pubDate><guid>https://ibm.github.io/ibmcloud-hpc-tutorials/04-custom-image/02-customize-software-on-the-vsi/</guid><description>From your local machine, login to the newly created VSI by running the following command (Note: After the VSI is in running state, you might still need to wait a few minutes before you can connect to it via ssh).
ssh -J root@&amp;lt;jump-node-ip&amp;gt; root@&amp;lt;new-instance-ip&amp;gt; You should find the jump-node-ip from the ssh login command generated while creating the existing LSF cluster.
Now customize the software on the VSI boot volume.</description></item><item><title>Set up VPN for the on-premise cluster</title><link>https://ibm.github.io/ibmcloud-hpc-tutorials/03-bursting/02-onprem-vpn/</link><pubDate>Sun, 02 Jan 2022 01:00:00 -0100</pubDate><guid>https://ibm.github.io/ibmcloud-hpc-tutorials/03-bursting/02-onprem-vpn/</guid><description>The following figure is an example of a VPN deployment among the two cluster.
In this step, however, we will create a VPN gateway for OnPremCluster, but will NOT create a VPN connection yet. The two VPN gateways from the two clusters must know each other&amp;rsquo;s gateway IP address and CIDR. For tutorial&amp;rsquo;s purpose we will rely on the cloud&amp;rsquo;s control plane to assign new VPN gateway IPs. As a result, the actual VPN connection must be made after both VPN gateways are created, to avoid a chicken-and-egg situation.</description></item><item><title>Create Image From Customized Volume</title><link>https://ibm.github.io/ibmcloud-hpc-tutorials/04-custom-image/03-create-image-from-customized-volume/</link><pubDate>Mon, 24 Jan 2022 15:57:51 -0500</pubDate><guid>https://ibm.github.io/ibmcloud-hpc-tutorials/04-custom-image/03-create-image-from-customized-volume/</guid><description>Now, we go back to the Cloud Shell for creating the new custom image.
Stop the VSI ibmcloud is instance-stop ${CUSTOM_IMG_VSI_NAME} Submit the request to create the custom image # Store Volume ID in a environment variable SOURCE_VOLUME_ID=$(ibmcloud is instance ${CUSTOM_IMG_VSI_NAME} --output JSON | jq -r .volume_attachments[0].volume.id) # Submit request to create image from the volume ibmcloud is image-create ${CUSTOM_IMG_NAME} --source-volume ${SOURCE_VOLUME_ID} This step can take some time depending on the image size.</description></item><item><title>Create a second cluster and establish VPN connection</title><link>https://ibm.github.io/ibmcloud-hpc-tutorials/03-bursting/03-cloud_cluster_step/</link><pubDate>Wed, 05 Jan 2022 01:00:00 -0100</pubDate><guid>https://ibm.github.io/ibmcloud-hpc-tutorials/03-bursting/03-cloud_cluster_step/</guid><description>We can now proceed to create the new cluster (&amp;ldquo;HPCCluster&amp;rdquo;) as the cloud bursting target in the US-East region (Washington DC). Using the LSF tile, the process is mostly identical as before, but with the following specific parameters:
Region (all occurances): us-east Zone: us-east-3 vpc_name: leave empty vpn_enabled: true vpn_peer_address: OnPremCluster&amp;rsquo;s VPN gateway IP vpn_peer_cidrs: OnPremCluster&amp;rsquo;s CIDR vpn_preshared_key: any string of user&amp;rsquo;s choice, to be used again in VPN connection management_node_count=1 worker_node_min_count=2 worker_node_max_count=2 Once all the resources are created, take note of the jump host and management node addresses.</description></item><item><title>Create Cluster From Custom Image</title><link>https://ibm.github.io/ibmcloud-hpc-tutorials/04-custom-image/04-create-cluster-from-custom-image/</link><pubDate>Mon, 24 Jan 2022 15:58:39 -0500</pubDate><guid>https://ibm.github.io/ibmcloud-hpc-tutorials/04-custom-image/04-create-cluster-from-custom-image/</guid><description>The last step is to use the newly created image to create a new LSF cluster. We are working on adding the description for how to do so and we will add it very soon.</description></item><item><title>Set up LSF multi-cluster and job forwarding</title><link>https://ibm.github.io/ibmcloud-hpc-tutorials/03-bursting/04-multicluster/</link><pubDate>Thu, 06 Jan 2022 01:00:00 -0100</pubDate><guid>https://ibm.github.io/ibmcloud-hpc-tutorials/03-bursting/04-multicluster/</guid><description>We now proceed to set up the LSF multi-cluster and job forwarding. Both clusters need to recognize each other, so you need to modify /opt/ibm/lsf/conf/lsf.shared. This configuration file should be identical in both clusters.
... Begin Cluster ClusterName Servers HPCCluster icgen2host-10-241-128-21 OnPremCluster icgen2host-10-240-128-21 End Cluster ... The two clusters are configured to have different lsb.queues files. In the HPCCluster, you need to append the following lines to /opt/ibm/lsf/conf/lsbatch/HPCCluster/configdir/lsb.queues to register a receive queue:</description></item><item><title>Prepare SSH and API Keys</title><link>https://ibm.github.io/ibmcloud-hpc-tutorials/02-setup-lsf-cluster/01-sshkey-api/</link><pubDate>Wed, 08 Dec 2021 10:01:27 -0500</pubDate><guid>https://ibm.github.io/ibmcloud-hpc-tutorials/02-setup-lsf-cluster/01-sshkey-api/</guid><description>Create Your SSL KEY key pair (if you don&amp;rsquo;t have one already) VSI access do not allow password-based authentication by default, so this step is essential for anyone to get access to the VSIs after they are created. If you do not have SSH keys on your machine where you plan to log in to your HPC cluster, use the ssh-keygen command to generate. This command is commonly available in modern operating systems and can be run by any user.</description></item><item><title>Create Your Cluster using IBM Spectrum LSF automation package</title><link>https://ibm.github.io/ibmcloud-hpc-tutorials/02-setup-lsf-cluster/02-hpc-cluster-catalog/</link><pubDate>Wed, 08 Dec 2021 10:05:34 -0500</pubDate><guid>https://ibm.github.io/ibmcloud-hpc-tutorials/02-setup-lsf-cluster/02-hpc-cluster-catalog/</guid><description>After you sign in to https://cloud.ibm.com/ with your account, search &amp;ldquo;HPC&amp;rdquo; or &amp;ldquo;Spectrum LSF&amp;rdquo; in IBM Cloud catalog. Find &amp;ldquo;IBM Spectrum LSF&amp;rdquo; and select the service. It will lead you to the HPC Cluster solution page. The tile can also be directly accessed from here.
Scroll down to the Configure your workspace section.
The resource group parameter in this section is where the Schematics workspace is provisioned on your IBM Cloud account.</description></item><item><title>Access Your Cluster</title><link>https://ibm.github.io/ibmcloud-hpc-tutorials/02-setup-lsf-cluster/03-access-cluster/</link><pubDate>Wed, 08 Dec 2021 10:03:16 -0500</pubDate><guid>https://ibm.github.io/ibmcloud-hpc-tutorials/02-setup-lsf-cluster/03-access-cluster/</guid><description>Overview of the HPC Cluster The HPC Cluster consists of a login node, a storage node where the block storage volume is being attached to, 1 to 3 LSF management nodes, and a number of LSF worker nodes.
The login node is served as a jump host and it is the only node which has the public IP address. Other nodes would only have private IP addresses and the only way to reach to these nodes is through the login node.</description></item><item><title>Run LSF Jobs Using Auto-Scaling</title><link>https://ibm.github.io/ibmcloud-hpc-tutorials/02-setup-lsf-cluster/04-run-job/</link><pubDate>Wed, 08 Dec 2021 10:03:30 -0500</pubDate><guid>https://ibm.github.io/ibmcloud-hpc-tutorials/02-setup-lsf-cluster/04-run-job/</guid><description>worker_node_max_count and _worker_node_min_count are the parameters for you to configure auto-scaling.
If you leave worker_node_max_count and _worker_node_min_count parameters with default values, you currently would have no worker node in your cluster.
In the example below, icgen2host-10-242-64-37 is the only node in your LSF cluser and is served as the LSF management node which is configured not to run users&amp;rsquo; workloads.
[lsfadmin@icgen2host-10-242-64-37 ~]$ bhosts -w HOST_NAME STATUS JL/U MAX NJOBS RUN SSUSP USUSP RSV icgen2host-10-242-64-37 closed_Full - 0 0 0 0 0 0 In addition, since the worker_node_max_count parameter is set to 10, you can only submit jobs that use no more than 10 nodes.</description></item><item><title>Manage your Cluster</title><link>https://ibm.github.io/ibmcloud-hpc-tutorials/02-setup-lsf-cluster/05-manage-cluster/</link><pubDate>Wed, 08 Dec 2021 10:03:39 -0500</pubDate><guid>https://ibm.github.io/ibmcloud-hpc-tutorials/02-setup-lsf-cluster/05-manage-cluster/</guid><description>You can manage your VPC resources in the Schematics workspace. You can use Resources in the menu on the left to visualize all the resources created for your HPC cluster. The cluster configuration parameters will be listed when you click Settings. When you are ready to destroy your cluster, you can use Actions &amp;gt; Destroy resources to destroy your cluster. If you no longer want to keep the workspace, you can select Delete workspace instead.</description></item><item><title>Conferences Workshops</title><link>https://ibm.github.io/ibmcloud-hpc-tutorials/conferences/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ibm.github.io/ibmcloud-hpc-tutorials/conferences/</guid><description>List of workshops given at confernces Name Description reInvent 2021 AWS ParallelClusterAPI Introduction to the AWS ParallelCluster API and the PCluster Manager web UI to manage clusters Supercomputing 2021 Tutorial Collection of labs created for the full-day tutorial delivered by the AWS HPC team at SC21 Supercomputing 2020 Tutorial Find the labs created for the full-day tutorial delivered by the AWS HPC team at SC20 Supercomputing 2019 Tutorial Collection of labs created for the first full-day tutorial delivered by the AWS HPC team at SC19</description></item><item><title>Credits</title><link>https://ibm.github.io/ibmcloud-hpc-tutorials/authors/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ibm.github.io/ibmcloud-hpc-tutorials/authors/</guid><description>Main Authors Lixiang Luo - site build, editor Hui-fang Wen - LSF foundation Chih Chieh Yang - LSF foundation Takeshi Yoshimura - LSF multi-cluster Additional Authors I-hsin Chung Ming Hung Chen Constantinos Evangelinos - file system topcis Dale Pearson Seetharami Seelam Suraksha Vidyarthi Robert Walkup - HPC performance tuning Website Design Christopher Hough - IBM theme</description></item><item><title>External Workshops</title><link>https://ibm.github.io/ibmcloud-hpc-tutorials/external/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ibm.github.io/ibmcloud-hpc-tutorials/external/</guid><description>List of External workshops Name Description Advanced Slurm on ParallelCluster This workshop shows advanced techniques with Slurm on ParallelCluster, including federation, accounting, and using the Slurm REST API with Jupyter notebooks Setting up a client library for the Slurm REST API A Jupyter notebook walkthrough of building and using a Python client library for the Slurm REST API</description></item></channel></rss>