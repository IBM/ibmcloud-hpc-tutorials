[{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/04-custom-image/01-create-vsi-from-default-lsf-image/","title":"Create VSI From Default LSF Image","tags":[],"description":"","content":"Start IBM Cloud Shell from the browser Find the icon to lauch Cloud Shell in the dash board:\nIt will create another browser window with the Cloud Shell:\nIn the Cloud Shell, follow the next steps to create a VSI (assuming bash):\n Assign values to environment variables for commands later  # Replace CLUSTER_PREFIX with the one you used to create the existing LSF cluster # Replace KEY_NAME, RESOURCE_GROUP_NAME, ZONE_NAME and CLUSTER_IMG_ID as needed CLUSTER_PREFIX=\u0026lt;cluster-name-prefix\u0026gt; CLUSTER_VPC_NAME=${CLUSTER_PREFIX}-vpc CLUSTER_SUBNET_NAME=${CLUSTER_PREFIX}-subnet CLUSTER_IMG_ID=r006-68478a2e-4abc-4bfb-9e4f-a6fb3b9b235f CLUSTER_SG_NAME=${CLUSTER_PREFIX}-sg CUSTOM_IMG_NAME=${CLUSTER_PREFIX}-custom-img CUSTOM_IMG_VSI_NAME=${CUSTOM_IMG_NAME}-vsi ZONE_NAME=us-south-3 PROFILE_NAME=bx2-2x8 KEY_NAME=ccyang-pub-key RESOURCE_GROUP_NAME=Default The CLUSTER_IMG_ID can be found from the open-source terraform script in IBM Cloud hpc-cluster-lsf github repository. You can choose the OS you wish to use. You should make sure the region key is the same as where you will create the VSI and where the custom image should be.\nCreate a VSI  ibmcloud is instance-create \\ ${CUSTOM_IMG_VSI_NAME} \\ ${CLUSTER_VPC_NAME} \\ ${ZONE_NAME} \\ ${PROFILE_NAME} \\ ${CLUSTER_SUBNET_NAME} \\ --image ${CLUSTER_IMG_ID} \\ --keys ${KEY_NAME} \\ --resource-group-name ${RESOURCE_GROUP_NAME} \\ --sgs ${CLUSTER_SG_NAME} Use the following command to check the status of the new VSI. Wait till the instance is in running state.  ibmcloud is instance ${CUSTOM_IMG_VSI_NAME} Get assigned IP address  ibmcloud is instance ${CUSTOM_IMG_VSI_NAME} --output JSON | jq -r .network_interfaces[0].primary_ipv4_address Keep the Cloud Shell open, we will reuse it in step 03 to create the custom image.\n"},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/03-bursting/01-onprem_cluster/","title":"Emulate an on-premise cluster","tags":["Cloud bursting"],"description":"","content":"The first step is to create an LSF cluster in IBM Cloud US-South (Dallas) region, which is to be used as an emulation of a \u0026ldquo;on-premise\u0026rdquo; cluster.\nIf you have not created an LSF cluster in us-south, you can repeat the steps in Standing up and LSF Cluster. For simplicity, please use the following setup:\n management_node_count=1 worker_node_min_count=2 worker_node_max_count=2  Remember, if you want to start over, always do a Destroy resources from the Workspace interface first, to make sure no zombie resources are left behind. A common issue after re-creating a cluster on the same region is that the IP ranges are often reused, so SSH will complain about dirty keys. Please follow the SSH\u0026rsquo;s prompts to resolve the issue.\nWe can login and check the cluster\u0026rsquo;s name once it is created:\n[lsfadmin@icgen2host-10-240-128-21 ~]$ lsclusters CLUSTER_NAME STATUS MASTER_HOST ADMIN HOSTS SERVERS HPCCluster ok icgen2host-10-240- lsfadmin 3 3 Note that the current LSF offering on IBM Cloud always creates a cluster named \u0026ldquo;HPCCluster\u0026rdquo;. For a multi-cluster setup, this would create confusion among clusters, so we need to rename the default cluster name \u0026ldquo;HPCCluster\u0026rdquo; to something else (\u0026ldquo;OnPremCluster\u0026rdquo;).\n[lsfadmin@icgen2host-10-240-128-21 ~]$ lsfshutdown Shutting down all server batch daemons ... Shut down server batch daemon on all the hosts? [y/n] y Shut down server batch daemon on \u0026lt;icgen2host-10-240-128-21\u0026gt; ...... done Shut down server batch daemon on \u0026lt;icgen2host-10-240-128-22\u0026gt; ...... done Shut down server batch daemon on \u0026lt;icgen2host-10-240-128-23\u0026gt; ...... done Shutting down all RESes ... Do you really want to shut down RES on all hosts? [y/n] y Shut down RES on \u0026lt;icgen2host-10-240-128-21\u0026gt; ...... done Shut down RES on \u0026lt;icgen2host-10-240-128-22\u0026gt; ...... done Shut down RES on \u0026lt;icgen2host-10-240-128-23\u0026gt; ...... done Shutting down all LIMs ... Do you really want to shut down LIMs on all hosts? [y/n] y Shut down LIM on \u0026lt;icgen2host-10-240-128-21\u0026gt; ...... done Shut down LIM on \u0026lt;icgen2host-10-240-128-22\u0026gt; ...... done Shut down LIM on \u0026lt;icgen2host-10-240-128-23\u0026gt; ...... done This shuts down all the LSF daemons throughout the cluster.\nNow we can safely rename the cluster:\nmv /opt/ibm/lsf/conf/lsf.datamanager.HPCCluster /opt/ibm/lsf/conf/lsf.datamanager.OnPremCluster mv /opt/ibm/lsf/conf/lsf.cluster.HPCCluster /opt/ibm/lsf/conf/lsf.cluster.OnPremCluster mv /opt/ibm/lsf/conf/lsbatch/HPCCluster /opt/ibm/lsf/conf/lsbatch/OnPremCluster mv /opt/ibm/lsf/conf/ego/HPCCluster /opt/ibm/lsf/conf/ego/OnPremCluster mv /opt/ibm/lsf/work/HPCCluster /opt/ibm/lsf/work/OnPremCluster mv /opt/ibm/lsf/work/OnPremCluster/live_confdir/lsbatch/HPCCluster /opt/ibm/lsf/work/OnPremCluster/live_confdir/lsbatch/OnPremCluster for fn in $(grep -r -e HPCCluster /opt/ibm/lsf/conf -lr | xargs ); do sed -i \u0026#39;s/HPCCluster/OnPremCluster/g\u0026#39; $fn; done find /opt/ibm/lsf/work/ /opt/ibm/lsf/log/ -type f -delete Finally, restart the LSF daemons. On the management node:\n[lsfadmin@icgen2host-10-240-128-21 ~]$ sudo lsf_daemons start Starting the LSF subsystem From your local computer:\n$ ssh -J root@52.118.83.233 root@10.240.128.22 lsf_daemons start Starting the LSF subsystem $ ssh -J root@52.118.83.233 root@10.240.128.23 lsf_daemons start Starting the LSF subsystem From the management node, we can confirm the cluster has been renamed and operational:\n[lsfadmin@icgen2host-10-240-128-21 ~]$ lsclusters CLUSTER_NAME STATUS MASTER_HOST ADMIN HOSTS SERVERS OnPremCluster ok icgen2host-10-240- lsfadmin 3 3 [lsfadmin@icgen2host-10-240-128-21 ~]$ bhosts HOST_NAME STATUS JL/U MAX NJOBS RUN SSUSP USUSP RSV icgen2host-10-240- closed - 0 0 0 0 0 0 icgen2host-10-240- ok - 4 0 0 0 0 0 icgen2host-10-240- ok - 4 0 0 0 0 0 "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/01-hpc-overview/","title":"HPC on Cloud Overview","tags":[],"description":"","content":"High Performance Computing (HPC) in the IBM Cloud can accelerate results by scaling a vast number of tasks and reduce costs by optimizing the right technology to meet your specifc goals. The cloud is always ready whenever you are ready to scale up. Whether you need a single CPU core or tens of thousands cores, a traditional MPI-oriented cluster or fully containerized Openshift cluster, IBM Cloud has the solution to meet the needs.\nFor traditional MPI-centric HPC workloads, the most natural way to utilize cloud resources is the Virtual Private Cloud (VPC). Essentially, it provides a set of configurable resources for building a secure cluster on a public cloud. In its most essential form, a virtual private cloud consists of a number of virtual machine instances, connected through a software-define network which is isolated from other VPCs on the same public cloud. VPCs are a \u0026ldquo;best of both worlds\u0026rdquo; approach to cloud computing. They give customers many of the advantages of private clouds, while leveraging public cloud resources and savings.\n   Component Traditional HPC cluster VPC cluster     Unit of compute node A bare-metal node, fixed configuration A virtual machine instance (VSI), flexible   Interconnect Physical Ethernet, Infiniband, etc. Software defined network    On this website we focus exclusively on IBM Cloud Gen2 VPC. In such context, a \u0026ldquo;VPC\u0026rdquo; resource actually refers to the software-defined network at a specific datacenter. Unless otherwise indicated, the term \u0026ldquo;VPC\u0026rdquo; in this website always referred to this narrow definition. Each VPC consists of one or multiple subnets, each of which is mapped to a zone within that datacenter. Once the VPC is established, VSIs can be created within subnets. Such process is the cloud analog to installing a physical cluster - putting physical computer nodes inside a server room and connect them with networks so that they can be accessed from outside.\n Region A region is an abstraction that is related to the geographic area in which a VPC is deployed. VPC can span multiple zones in a region. Example: us-south\nZone A zone is an abstraction that refers to the physical data center that hosts the compute, network, and storage resources, as well as the related cooling and power, which provides services and applications. Zones are isolated from each other. Example: us-south-1/2/3\nSecurity Groups (SG) Set of rules to filter traffic to an instance.\nSubnet Each subnet consists of a specified IP address range (CIDR block). Subnets are bound to a single zone and cannot span multiple zones or regions. Subnets within the VPC offer private connectivity; they can talk to each other over a private link through the implicit router. Setting up routes is not necessary.\nPublic Gateway A Public Gateway enables a subnet and all its attached virtual server instances to connect to the internet.\nFloating IP Floating IP addresses are IP addresses that are provided by the system and are reachable from the public internet.\nVirtual Server Instance (VSI) A VSI is basically a virtual machine instance, created from a shared VSI profile (template, or base image) provided by the cloud. An VSI have multiple states (running, stopped, etc.) during its life cycle.\nWhile it is possible to manually build a cluster from these componenets, there is a much easier way to deploy a HPC-oriented cluster on IBM Cloud. In fact, Most of the tutorials on this website are based on the automation package for Spectrum LSF. IBM Spectrum LSF is widely used to manage parallel distributed HPC workloads. The automation package deploys LSF management and compute hosts with a shared filesystem. It also configures auto-scaling of compute hosts with its resource connector for IBM Cloud. The deployed cluster runs the Red Hat Enterprise Linux 7.7 with pre-installed software packages such as OpenMPI.\nThe automation package enables users to quickly create an HPC cluster on IBM Cloud. Users can import data to the shared filesystem and submit HPC jobs using the bsub command. The automation can be regarded as a quick starter kit for HPC in the cloud, but also it enables cloud bursting, a technique to deal with the temporal insufficiency of computing resources at an on-premise cluster using the elastic cloud infrastructures.\n"},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/04-custom-image/02-customize-software-on-the-vsi/","title":"Customize Software on the VSI","tags":[],"description":"","content":"From your local machine, login to the newly created VSI by running the following command (Note: After the VSI is in running state, you might still need to wait a few minutes before you can connect to it via ssh).\nssh -J root@\u0026lt;jump-node-ip\u0026gt; root@\u0026lt;new-instance-ip\u0026gt; You should find the jump-node-ip from the ssh login command generated while creating the existing LSF cluster.\nNow customize the software on the VSI boot volume.\n"},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/03-bursting/02-onprem-vpn/","title":"Set up VPN for the on-premise cluster","tags":["Cloud bursting"],"description":"","content":"The following figure is an example of a VPN deployment among the two cluster.\n In this step, however, we will create a VPN gateway for OnPremCluster, but will NOT create a VPN connection yet. The two VPN gateways from the two clusters must know each other\u0026rsquo;s gateway IP address and CIDR. For tutorial\u0026rsquo;s purpose we will rely on the cloud\u0026rsquo;s control plane to assign new VPN gateway IPs. As a result, the actual VPN connection must be made after both VPN gateways are created, to avoid a chicken-and-egg situation.\nTo find the CIDR, go to the VPC created by LSF automation package (\u0026ldquo;LSF tile\u0026rdquo;). Since us-south-3 is used, the corresponding IP range is 10.240.128.0/18.\n Now select VPC Infrastructure \u0026gt; VPNs. In the \u0026ldquo;VPNs for VPC\u0026rdquo; page, make sure Dallas is selected as the region, and then click Create. We need to use the following setup:\n VPN type: Site-to-site gateway Resource group: hpca_resgrp Virtual private cloud: the VPC used by the OnPremCluster Subnet: this should be automatically selected since there is only one subnet was created in OnPremCluster\u0026rsquo;s VPC Mode: Policy-based VPN connection for VPC: deselected  Upon clicking Create VPN gateway, it takes a moment for the gateway IP to be allocated and reveal in the VPN page. This is the gateway IP we will need during the creation of the next cluster.\n "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/02-setup-lsf-cluster/","title":"Standing up an LSF Cluster","tags":["Introduction","LSF"],"description":"","content":"IBM Spectrum LSF automation package is part of the HPC Clustering Services on IBM Cloud to enable users to stand up a HPC environment within minutes. LSF is an advanced workload management solution for demanding HPC environments. HPC cluster deployment starts with the definition of the desired cluster configuration; for example, count of management nodes, worker nodes, amount of storage and so on. Open-source technologies like Terraform and Ansible have been used to automate the provisioning and configuration of all associated IBM Cloud VPC resources and LSF.\nIn this section, you are using the IBM Spectrum LSF catalog tile (\u0026ldquo;LSF tile\u0026rdquo; for short) to create an HPC cluster in IBM Cloud and run an LSF job as you would on-premises. This includes the following steps:\n Prepare your SSH key and IBM Cloud API keys. Configure cluster parameters from your IBM Cloud web console. Create your first cluster. Submit a sample job and check what is happening in the background. Delete the cluster.  "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/04-custom-image/03-create-image-from-customized-volume/","title":"Create Image From Customized Volume","tags":[],"description":"","content":"Now, we go back to the Cloud Shell for creating the new custom image.\n Stop the VSI  ibmcloud is instance-stop ${CUSTOM_IMG_VSI_NAME} Submit the request to create the custom image  # Store Volume ID in a environment variable SOURCE_VOLUME_ID=$(ibmcloud is instance ${CUSTOM_IMG_VSI_NAME} --output JSON | jq -r .volume_attachments[0].volume.id) # Submit request to create image from the volume ibmcloud is image-create ${CUSTOM_IMG_NAME} --source-volume ${SOURCE_VOLUME_ID} This step can take some time depending on the image size. You can check the webpage of the images (Navigation Menu \u0026ndash;\u0026gt; VPC Infrastructure \u0026ndash;\u0026gt; Custom images) for the status of the image creation.\nWhen the image is created, we can delete the VSI that is no longer needed  ibmcloud is instance-delete ${CUSTOM_IMG_VSI_NAME} "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/03-bursting/03-cloud_cluster_step/","title":"Create a second cluster and establish VPN connection","tags":["Cloud bursting"],"description":"","content":"We can now proceed to create the new cluster (\u0026ldquo;HPCCluster\u0026rdquo;) as the cloud bursting target in the US-East region (Washington DC). Using the LSF tile, the process is mostly identical as before, but with the following specific parameters:\n Region (all occurances): us-east Zone: us-east-3 vpc_name: leave empty vpn_enabled: true vpn_peer_address: OnPremCluster\u0026rsquo;s VPN gateway IP vpn_peer_cidrs: OnPremCluster\u0026rsquo;s CIDR vpn_preshared_key: any string of user\u0026rsquo;s choice, to be used again in VPN connection management_node_count=1 worker_node_min_count=2 worker_node_max_count=2  Once all the resources are created, take note of the jump host and management node addresses. Head to the VPN section and find its corresponding VPN\u0026rsquo;s gateway IP and CIDR. Also we need to make sure HPCCluster\u0026rsquo;s VPN is enabled.\nSuppose we end up with the following setup from the two clusters:\n    OnPremCluster HPCCluster     Management node icgen2host-10-240-128-21 (10.240.128.21) icgen2host-10-240-128-21 (10.241.128.21)   VPN gateway 52.118.79.73 150.239.220.255   CIDR 10.240.128.0/18 10.241.128.0/18   Preshared key n/a lfsclustervpn    The next step is to establish the VPN connection from OnPremCluster VPN. As shown below, \u0026ldquo;Local\u0026rdquo; corresponds to OnPremCluster and \u0026ldquo;Peer\u0026rdquo; corresponds to HPCCluster. The same \u0026ldquo;preshared key\u0026rdquo; used for creating the HPCCluster\u0026rsquo;s VPN must be used.\n Upon creation, it take about a minute before the VPN connection becomes active, as shown below:\n Unlike HPCCluster\u0026rsquo;s subnet, OnPremCluster\u0026rsquo;s subnet is not created without an attached VPN. Its UDP ports 500 and 4500 must be manually made accessible from outside. This is done by creating an open inbound rule in OnPremCluster\u0026rsquo;s attached security group, as shown below:\n We also need to modify /etc/hosts on the management node on both cluster, so that the hostnames for the LSF masters are DNS-resolvable. On OnPremCluster:\n... 10.240.128.21 icgen2host-10-240-128-21 ... 10.241.128.21 icgen2host-10-241-128-21 # added On HPCCluster:\n... 10.241.128.21 icgen2host-10-241-128-21 ... 10.240.128.21 icgen2host-10-240-128-21 # added To confirm VPN connection from OnPremCluster:\n[lsfadmin@icgen2host-10-240-128-21 ~]$ ping icgen2host-10-241-128-21 PING icgen2host-10-241-128-21 (10.241.128.21) 56(84) bytes of data. 64 bytes from icgen2host-10-241-128-21 (10.241.128.21): icmp_seq=1 ttl=62 time=33.1 ms ... "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/03-bursting/","title":"Cloud Bursting","tags":["Bursting","Introduction"],"description":"","content":"LSF can support cloud bursting using multi-cluster setups and job forwarding. With these features, LSF can forward jobs from on-premise to a cloud cluster when the on-premise cannot satisfy requests for computing resources. On the other hand, the cloud cluster can be scaled down to save the costs for compute nodes during idle time.\n The automation package also supports the configuration of a virtual private network (VPN) for an LSF cluster. VPN enables secure network connections between on-premise and cloud clusters as if they were directly connected. On top of the VPN connectivity, two LSF clusters can securely share cluster information and forward jobs over the internet.\nThis tutorial section consists of these parts:\n Reconfiguring an existing LSF cluster on IBM Cloud to emulate an private \u0026ldquo;on-premise cluster\u0026rdquo; with its own VPN server Set up VPN for the on-premise cluster Creating a new LSF cluster on IBM Cloud, which is the target of the workload bursting Setting up VPN connection between these two cluster Setting up multi-cluster and trying cloud bursting with dummy workloads  "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/04-custom-image/04-create-cluster-from-custom-image/","title":"Create Cluster From Custom Image","tags":[],"description":"","content":"The last step is to use the newly created image to create a new LSF cluster. We are working on adding the description for how to do so and we will add it very soon.\n"},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/04-custom-image/","title":"Custom Image (Under Construction)","tags":["HPC","LSF","Optional"],"description":"","content":"In this tutorial, we will guide you through the steps to create a custom image that allows you to install custom software packages and later reuse the image for creating new LSF clusters. We assume you have followed the steps in II - Standing up an LSF Cluster to create an existing LSF cluster, and we will reuse its VPC infrastructure to create a VSI within it.\n"},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/03-bursting/04-multicluster/","title":"Set up LSF multi-cluster and job forwarding","tags":["Cloud bursting"],"description":"","content":"We now proceed to set up the LSF multi-cluster and job forwarding. Both clusters need to recognize each other, so you need to modify /opt/ibm/lsf/conf/lsf.shared. This configuration file should be identical in both clusters.\n... Begin Cluster ClusterName Servers HPCCluster icgen2host-10-241-128-21 OnPremCluster icgen2host-10-240-128-21 End Cluster ... The two clusters are configured to have different lsb.queues files. In the HPCCluster, you need to append the following lines to /opt/ibm/lsf/conf/lsbatch/HPCCluster/configdir/lsb.queues to register a receive queue:\n... Begin Queue QUEUE_NAME=recv_q RCVJOBS_FROM=OnPremCluster PRIORITY=30 NICE=20 RC_HOSTS=all End Queue ... OnPremCluster is configured to have a send queue at /opt/ibm/lsf/conf/lsbatch/OnPremiseCluster/configdir/lsb.queues:\n... Begin Queue QUEUE_NAME=send_q SNDJOBS_TO=recv_q@allclusters PRIORITY=30 NICE=20 End Queue ... Restart both clusters by running the following command:\n$ lsfrestart Check if two clusters are connected\n[lsfadmin@icgen2host-10-240-128-21 ~]$ lsclusters -w CLUSTER_NAME STATUS MASTER_HOST ADMIN HOSTS SERVERS OnPremCluster ok icgen2host-10-240-128-21 lsfadmin 3 3 HPCCluster ok icgen2host-10-241-128-21 lsfadmin 3 3 Now you can forward jobs from OnPremCluster to HPCCluster. If we submit 20 concurrent sleep 10s jobs, some of the jobs will be scheduled on HPCCluster:\n$ for i in `seq 1 20`; do bsub -q send_q sleep 10s; done after a few seconds:\n[lsfadmin@icgen2host-10-240-128-21 ~]$ bjobs -w JOBID USER STAT QUEUE FROM_HOST EXEC_HOST JOB_NAME SUBMIT_TIME 397 lsfadmin RUN send_q icgen2host-10-240-128-21 icgen2host-10-240-128-23 sleep 10s Mar 30 21:25 398 lsfadmin RUN send_q icgen2host-10-240-128-21 icgen2host-10-240-128-23 sleep 10s Mar 30 21:25 399 lsfadmin RUN send_q icgen2host-10-240-128-21 icgen2host-10-240-128-23 sleep 10s Mar 30 21:25 400 lsfadmin RUN send_q icgen2host-10-240-128-21 icgen2host-10-240-128-23 sleep 10s Mar 30 21:25 401 lsfadmin RUN send_q icgen2host-10-240-128-21 icgen2host-10-240-128-22 sleep 10s Mar 30 21:25 402 lsfadmin RUN send_q icgen2host-10-240-128-21 icgen2host-10-240-128-22 sleep 10s Mar 30 21:25 403 lsfadmin RUN send_q icgen2host-10-240-128-21 icgen2host-10-240-128-22 sleep 10s Mar 30 21:25 404 lsfadmin RUN send_q icgen2host-10-240-128-21 icgen2host-10-240-128-22 sleep 10s Mar 30 21:25 405 lsfadmin RUN send_q icgen2host-10-240-128-21 icgen2host-10-241-128-23@HPCCluster sleep 10s Mar 30 21:25 406 lsfadmin RUN send_q icgen2host-10-240-128-21 icgen2host-10-241-128-23@HPCCluster sleep 10s Mar 30 21:25 407 lsfadmin RUN send_q icgen2host-10-240-128-21 icgen2host-10-241-128-23@HPCCluster sleep 10s Mar 30 21:25 408 lsfadmin RUN send_q icgen2host-10-240-128-21 icgen2host-10-241-128-23@HPCCluster sleep 10s Mar 30 21:25 409 lsfadmin RUN send_q icgen2host-10-240-128-21 icgen2host-10-241-128-22@HPCCluster sleep 10s Mar 30 21:25 410 lsfadmin RUN send_q icgen2host-10-240-128-21 icgen2host-10-241-128-22@HPCCluster sleep 10s Mar 30 21:25 411 lsfadmin RUN send_q icgen2host-10-240-128-21 icgen2host-10-241-128-22@HPCCluster sleep 10s Mar 30 21:25 412 lsfadmin RUN send_q icgen2host-10-240-128-21 icgen2host-10-241-128-22@HPCCluster sleep 10s Mar 30 21:25 413 lsfadmin PEND send_q icgen2host-10-240-128-21 - sleep 10s Mar 30 21:25 414 lsfadmin PEND send_q icgen2host-10-240-128-21 - sleep 10s Mar 30 21:25 415 lsfadmin PEND send_q icgen2host-10-240-128-21 - sleep 10s Mar 30 21:25 416 lsfadmin PEND send_q icgen2host-10-240-128-21 - sleep 10s Mar 30 21:25 "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/02-setup-lsf-cluster/01-sshkey-api/","title":"Prepare SSH and API Keys","tags":["LSF"],"description":"","content":"Create Your SSL KEY key pair (if you don\u0026rsquo;t have one already) VSI access do not allow password-based authentication by default, so this step is essential for anyone to get access to the VSIs after they are created. If you do not have SSH keys on your machine where you plan to log in to your HPC cluster, use the ssh-keygen command to generate. This command is commonly available in modern operating systems and can be run by any user. In Linux, for example:\n$ ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/home/someone/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/someone/.ssh/id_rsa. Your public key has been saved in /home/someone/.ssh/id_rsa.pub. Copy and paste the content in /home/someone/.ssh/id_rsa.pub to the public key field in Step 2 below.\nCreate Your IBM Cloud SSH Key This step allows you to upload the public key of your existing public-private SSL key pair. The public key then becomes a managed resource on IBM Cloud, which can be referred to by its resource id during creation processes of other resources, such as VSIs.\nWhen selecting the region, please use one of these 4 regions\n us-south (Dallas) us-east (Washington DC) eu-gb (London) eu-de (Frankfurt)  After you sign in to https://cloud.ibm.com/ with your account, start with the Navigation Menu on the dashboard in the top left corner of the page:\n  Click VPC Infrastructure \u0026gt; SSH Keys\n  Click Create\n   Add name (e.g. demo-ssh-key), type your resource group name and select region Copy the public key and paste it in the public_key field(e.g.: contents in .ssh/id_rsa.pub)  Click on Add SSH Key  Please remember your ssh key name (e.g., demo-ssh-key), the resource group name and the region you have selected. You will need to use those in the next step when creating a cluster.\nCreate Your IBM Cloud API Key This step allows you to create an API key from IBM Cloud. The API key is usually kept on the user\u0026rsquo;s local machine, which can be used as an authentication mechanism for IBM Cloud API calls. Without the API key, many API calls would require you to re-authenticate with other, more cumbersome, mechanisms. API keys are not bound to any specific datacenter.\n  Go to Manage \u0026gt; Access (IAM)\n  Click API keys\n  Click Create an IBM Cloud API key\n3.1. Enter a name for your API key and Click Create\n3.2. Then, click Show to display the API key. Or, click Copy to copy and save it for later, or click Download   Please keep the API key saved in a secure place. You will need to use this in the next step when creating the cluster.\n"},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/02-setup-lsf-cluster/02-hpc-cluster-catalog/","title":"Create Your Cluster using IBM Spectrum LSF automation package","tags":["LSF"],"description":"","content":"After you sign in to https://cloud.ibm.com/ with your account, search \u0026ldquo;HPC\u0026rdquo; or \u0026ldquo;Spectrum LSF\u0026rdquo; in IBM Cloud catalog. Find \u0026ldquo;IBM Spectrum LSF\u0026rdquo; and select the service. It will lead you to the HPC Cluster solution page. The tile can also be directly accessed from here.\n Scroll down to the Configure your workspace section.\n The resource group parameter in this section is where the Schematics workspace is provisioned on your IBM Cloud account. The value for this parameter can be different than the one used for SSH key. You can leave the other fields with the default.\nScroll down to the Set the deployment values section. You will need to fill out three fields in this Parameters without default values section. You can toggle Use an existing secret to No for the api_key parameter and paste your api key saved earlier. Enter \u0026ldquo;true\u0026rdquo; for lsf_license_confirmatin and your ssh_key_name (e.g.: demo-ssh-key).\nContinue with the Parameters with default values section. Though you can leave most of the parameters with default values, you need to modify a few parameters depending on your cloud account and ssh key.\n Please select a cluster_prefix string that would be unique to you (e.g.: use your initial here). The resource group value needs to be consistent with the one which you use to create your SSH key. The region (may not be required in the future) and zone need to match where your ssh key is created in the previous step.  A list of supported regions: eu-de,eu-gb,us-east,us-south Each region has 3 zones (e.g.: us-east-1, us-east-2, us-east-3 for the us-east region)   Please just use 1 for management_node_count.  To learn more about cluster configuration parameters, please refer to this tutorial in Step 4.\nNow you are ready to create the cluster. In the bottom right corner of the page, select the license agreements button and click Install. It will lead to your Schematics workspace and will take few minutes for your cluster to be ready.\n "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/02-setup-lsf-cluster/03-access-cluster/","title":"Access Your Cluster","tags":["LSF"],"description":"","content":"Overview of the HPC Cluster The HPC Cluster consists of a login node, a storage node where the block storage volume is being attached to, 1 to 3 LSF management nodes, and a number of LSF worker nodes.\n   The login node is served as a jump host and it is the only node which has the public IP address. Other nodes would only have private IP addresses and the only way to reach to these nodes is through the login node. Users should log in to the primary LSF management node (or LSF master) and do most of the operations from the LSF master. By default, lsfadmin is the only user id being created on the cluster. The ssh passwordless setup is configured between the LSF master and workers. Users can reach to any other worker node with the lsfadmin user id from the LSF master.\n  The worker node can be a static resource. In this case its lifecycle is managed by Schematics/Terraform. Users can request a number of static worker nodes and these workers remain available in the LSF cluster until a Schematics/Terraform destroy action is being performed. The LSF Resource Connector functionality creates additional workers when there is not enough capacity to run jobs and destroys workers when the demands decrease. The lifecycle of these dynamic workers is managed by the LSF Resource Connector. Users should wait these dynamic resources returned to the Cloud before destroying the entire VPC cluster through Schematics/Terraform.\n  The storage node is configured as an NFS server and the block storage volume is mounted to /data which is exported to share with LSF cluster nodes. At the NSF client end, the LSF cluster nodes in this case, we mount the remote directory, /data, to /mnt/data locally. A soft link, /home/lsfadmin/shared, also points to /mnt/data. Users can use /home/lsfadmin/shared as a shared file system for their applications.\n  Schematics Workspace After you click Install, you will be taken to a Schematics workspace created in your Cloud account show as below. This is where you manage your HPC cluster. You can learn more about IBM Cloud Schematics Solution here.\nClick on Jobs and you will be updated with the progress of cluster creation. Once it\u0026rsquo;s completed, you will be given a ssh command towards the end of this log file. Go to the terminal where you have your SSH key and run the ssh command. You can use lsid to display the LSF cluster information.\n% ssh -J root@141.125.159.202 lsfadmin@10.242.64.37 Last login: Mon Dec 6 14:38:05 2021 from 10.242.64.4 [lsfadmin@icgen2host-10-242-64-37 ~]$ lsid IBM Spectrum LSF Standard 10.1.0.11, Jul 16 2021 Copyright International Business Machines Corp. 1992, 2016. US Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp. My cluster name is HPCCluster My master name is icgen2host-10-242-64-37 That\u0026rsquo;s it! You are on the LSF management node and the cluster is ready for you to run your HPC workloads.\n"},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/02-setup-lsf-cluster/04-run-job/","title":"Run LSF Jobs Using Auto-Scaling","tags":["LSF"],"description":"","content":"worker_node_max_count and _worker_node_min_count are the parameters for you to configure auto-scaling.\n If you leave worker_node_max_count and _worker_node_min_count parameters with default values, you currently would have no worker node in your cluster.\nIn the example below, icgen2host-10-242-64-37 is the only node in your LSF cluser and is served as the LSF management node which is configured not to run users\u0026rsquo; workloads.\n[lsfadmin@icgen2host-10-242-64-37 ~]$ bhosts -w HOST_NAME STATUS JL/U MAX NJOBS RUN SSUSP USUSP RSV icgen2host-10-242-64-37 closed_Full - 0 0 0 0 0 0 In addition, since the worker_node_max_count parameter is set to 10, you can only submit jobs that use no more than 10 nodes. Here, we submit a simple sleep job by requesting 2 worker nodes and observe how auto-scaling works behind the scene to dynamically provision two nodes and add to the existing cluster.\n[lsfadmin@icgen2host-10-242-64-37 ~]$ bsub -n 2 -R \u0026#34;span[ptile=1]\u0026#34; sleep 10 Job \u0026lt;3\u0026gt; is submitted to default queue \u0026lt;normal\u0026gt;. Wait for few minutes. Run bhosts again and you will see two new worker nodes added to the cluster by auto-scaling.\n[lsfadmin@icgen2host-10-242-64-37 ~]$ bhosts -w HOST_NAME STATUS JL/U MAX NJOBS RUN SSUSP USUSP RSV icgen2host-10-242-64-37 closed_Full - 0 0 0 0 0 0 icgen2host-10-242-64-38 ok - 4 0 0 0 0 0 icgen2host-10-242-64-40 ok - 4 0 0 0 0 0 The submitted job is shown as completed running the sleep command on these two new nodes.\n[lsfadmin@icgen2host-10-242-64-37 ~]$ bjobs -a -w JOBID USER STAT QUEUE FROM_HOST EXEC_HOST JOB_NAME SUBMIT_TIME 3 lsfadmin DONE normal icgen2host-10-242-64-37 icgen2host-10-242-64-38:icgen2host-10-242-64-40 sleep 10 Dec 7 11:55 icgen2host-10-242-64-40 After the new nodes have been idling for 10 minutes, auto-scaling will destroy the resources and have those removed from your cluster.\nNow let\u0026rsquo;s run a simple MPI job. Go to /home/lsfadmin/shared and this is the directory which is visible to all nodes in your cluster. Threfore, you should place your MPI workloads under this directory. OpenMPI 4.1 is already installed under /usr/local/openmpi-4.1.0 and you can include its bin/ directory to your PATH environment variable.\nlsfadmin@icgen2host-10-242-64-37:~\u0026gt;cd shared/ lsfadmin@icgen2host-10-242-64-37:~/shared\u0026gt;pwd /home/lsfadmin/shared lsfadmin@icgen2host-10-242-64-37:~/shared\u0026gt; export PATH=/usr/local/openmpi-4.1.0/bin:$PATH lsfadmin@icgen2host-10-242-64-37:~/shared\u0026gt;which mpicc /usr/local/openmpi-4.1.0/bin/mpicc Let\u0026rsquo;s create a MPI program named hello.c and build it using mpicc from OpenMPI 4.1.\nlsfadmin@icgen2host-10-242-64-37:~/shared\u0026gt;cat \u0026gt; hello.c \u0026lt;\u0026lt; EOF \u0026gt; #include \u0026lt;mpi.h\u0026gt; \u0026gt; #include \u0026lt;stdio.h\u0026gt; \u0026gt; #include \u0026lt;stdlib.h\u0026gt; \u0026gt; #include \u0026lt;unistd.h\u0026gt; \u0026gt; \u0026gt; int main(int argc, char *argv[]) \u0026gt; { \u0026gt; int myrank; \u0026gt; char hostname[50]; \u0026gt; \u0026gt; gethostname(hostname, 50); \u0026gt; printf(\u0026#34;Hello from %s (before MPI_Init)...\\n\u0026#34;, hostname); \u0026gt; \u0026gt; MPI_Init(\u0026amp;argc, \u0026amp;argv); \u0026gt; //sleep(600); \u0026gt; MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;myrank); \u0026gt; printf(\u0026#34;Hello from %s, rank %d...\\n\u0026#34;, hostname, myrank); \u0026gt; \u0026gt; MPI_Finalize(); \u0026gt; \u0026gt; return 0; \u0026gt; } \u0026gt; EOF lsfadmin@icgen2host-10-242-64-37:~/shared\u0026gt;mpicc -g -o hello hello.c We now run a MPI job with 4 MPI tasks on two nodes, each node with 2 MPI ranks.\nlsfadmin@icgen2host-10-242-64-37:~/shared\u0026gt;bsub -o %J.out -e %J.err -n 4 -R \u0026#34;span[ptile=2]\u0026#34; mpirun ./hello Job \u0026lt;5\u0026gt; is submitted to default queue \u0026lt;normal\u0026gt;. Check the 5.out file:\nlsfadmin@icgen2host-10-242-64-37:~/shared\u0026gt;cat 5.out Sender: LSF System \u0026lt;lsfadmin@icgen2host-10-242-64-38\u0026gt; Subject: Job 5: \u0026lt;mpirun ./hello\u0026gt; in cluster \u0026lt;HPCCluster\u0026gt; Done Job \u0026lt;mpirun ./hello\u0026gt; was submitted from host \u0026lt;icgen2host-10-242-64-37\u0026gt; by user \u0026lt;lsfadmin\u0026gt; in cluster \u0026lt;HPCCluster\u0026gt; at Tue Dec 7 13:43:30 2021 Job was executed on host(s) \u0026lt;2*icgen2host-10-242-64-38\u0026gt;, in queue \u0026lt;normal\u0026gt;, as user \u0026lt;lsfadmin\u0026gt; in cluster \u0026lt;HPCCluster\u0026gt; at Tue Dec 7 13:45:28 2021 \u0026lt;2*icgen2host-10-242-64-43\u0026gt; \u0026lt;/home/lsfadmin\u0026gt; was used as the home directory. \u0026lt;/home/lsfadmin/shared\u0026gt; was used as the working directory. Started at Tue Dec 7 13:45:28 2021 Terminated at Tue Dec 7 13:45:30 2021 Results reported at Tue Dec 7 13:45:30 2021 Your job looked like: ------------------------------------------------------------ # LSBATCH: User input mpirun ./hello ------------------------------------------------------------ Successfully completed. Resource usage summary: CPU time : 0.27 sec. Max Memory : - Average Memory : - Total Requested Memory : - Delta Memory : - Max Swap : - Max Processes : - Max Threads : - Run time : 4 sec. Turnaround time : 120 sec. The output (if any) follows: Hello from icgen2host-10-242-64-38 (before MPI_Init)... Hello from icgen2host-10-242-64-38 (before MPI_Init)... Hello from icgen2host-10-242-64-43 (before MPI_Init)... Hello from icgen2host-10-242-64-43 (before MPI_Init)... Hello from icgen2host-10-242-64-38, rank 0... Hello from icgen2host-10-242-64-38, rank 1... Hello from icgen2host-10-242-64-43, rank 2... Hello from icgen2host-10-242-64-43, rank 3... PS: Read file \u0026lt;5.err\u0026gt; for stderr output of this job. More information about LSF You can learn more about LSF here.\nBelow is a cheat sheet for useful LSF commands:\n "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/02-setup-lsf-cluster/05-manage-cluster/","title":"Manage your Cluster","tags":["LSF"],"description":"","content":"You can manage your VPC resources in the Schematics workspace. You can use Resources in the menu on the left to visualize all the resources created for your HPC cluster. The cluster configuration parameters will be listed when you click Settings. When you are ready to destroy your cluster, you can use Actions \u0026gt; Destroy resources to destroy your cluster. If you no longer want to keep the workspace, you can select Delete workspace instead. It has options to allow you to both destroy your cluster as well as your workspace.\n"},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/tags/hpc/","title":"HPC","tags":[],"description":"","content":""},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/tags/lsf/","title":"LSF","tags":[],"description":"","content":""},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/tags/optional/","title":"Optional","tags":[],"description":"","content":""},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/tags/","title":"Tags","tags":[],"description":"","content":""},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/tags/cloud-bursting/","title":"Cloud bursting","tags":[],"description":"","content":""},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/tags/bursting/","title":"Bursting","tags":[],"description":"","content":""},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/tags/introduction/","title":"Introduction","tags":[],"description":"","content":""},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/","title":"HPC on IBM Cloud Tutorials","tags":[],"description":"","content":"Welcome to HPCA \u0026ldquo;HPC on IBM Cloud\u0026rdquo; Tutorials Please secure your IBM Cloud account NOW HPC on IBM Cloud tutorial includes comprehensive access to IBM Cloud for all participants during and shortly after the event. It is highly recommended to secure your IBM Cloud account in advance in order to minimize setup time during the event. Please follow these steps to apply for your account:\n  (You will) signup for a new IBMid ONLY (note the email address you use), WITHOUT creating an IBM Cloud account. Link: https://www.ibm.com/account/reg/us-en/signup?formid=urx-19776\n  (You will) send us an email including the email address you used for your \u0026ldquo;new IBMid\u0026rdquo;:\n   Email recipient: LIXIANG.LUO@ibm.com Email title: [HPCA] Request for a hands-on account Email content: the email address you used for your \u0026ldquo;new IBMid\u0026rdquo;   (We will) invite your “new IBMid” to the hands-on account.\n  (You will) locate the invitation email sent from IBM Cloud and go through the sign-up process.\n  (You will) sign in IBM Cloud (cloud.ibm.com) using the “new IBMid”.\nIf everything goes as expected, you should see \u0026ldquo;2533666 – LIXIANG LUO’s Account\u0026rdquo; on the top navigation bar.\n   HPCA Tutorial Agenda Time: April 2 2022, Saturday, 9:00AM ~ 1:00PM, US Eastern Time\n   Time Topic     9:00 ~ 9:15 Introduction   9:15 ~ 9:30 Building blocks of cloud   9:30 ~ 9:45 IBM Cloud account setup   9:45 ~ 10:30 Stand up an HPC cluster on IBM Cloud   10:30 ~ 11:30 LSF-manged multi-cloud and cloudbursting   11:30 ~ 12:15 Storage solutions   12:15 ~ 12:45 Best practices for HPC on Cloud    We look forward to seeing you in the event!\n Beyond the HPCA event, this website will evolve into an open-source documentation project for \u0026ldquo;HPC on IBM Cloud\u0026rdquo; topics. We aim to provide tailored, step-by-step tutorials on various tasks in utilizing public cloud resources for HPC workloads. We hope to smooth the learning curve for the transition from traditional on-premise HPC clusters to cloud-based or even cloud-native workflows. Most information here are derived from references and tutorials from IBM Cloud\u0026rsquo;s online articles.\nTopics on this website:\n  Cloud Basics introduces the building blocks of cloud computing and basic preparations for going through the other topics.\n  Standing up an LSF Cluster shows how to set up a HPC cluster on IBM Cloud, including how to use the autoscaling capability.\n  Cloud Bursting explores the exciting possiblities of leveraging LSF-managed multi-cluster on hybrid cloud.\n  Custom Image explains how to create your own custom VSI images hosted on IBM Cloud.\n  (Upcoming) HPC File Systems discusses the options for a high-performance parallel file system on IBM Cloud.\n  (Upcoming) Performance tuning provides cloud-specific performance tuning suggestions for HPC applications.\n  Select a topic from the nagivation panel on the left or continue to HPC on Cloud Overview.\n"},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/conferences/","title":"Conferences Workshops","tags":[],"description":"","content":"List of workshops given at confernces    Name Description     reInvent 2021 AWS ParallelClusterAPI Introduction to the AWS ParallelCluster API and the PCluster Manager web UI to manage clusters   Supercomputing 2021 Tutorial Collection of labs created for the full-day tutorial delivered by the AWS HPC team at SC21   Supercomputing 2020 Tutorial Find the labs created for the full-day tutorial delivered by the AWS HPC team at SC20   Supercomputing 2019 Tutorial Collection of labs created for the first full-day tutorial delivered by the AWS HPC team at SC19    "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/authors/","title":"Credits","tags":[],"description":"","content":" I-hsin Chung Ming Hung Chen Constantinos Evangelinos Lixiang Luo Dale Pearson Seetharami Seelam Robert Walkup Hui-fang Wen Chih Chieh Yang  "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/external/","title":"External Workshops","tags":[],"description":"","content":"List of External workshops    Name Description     Advanced Slurm on ParallelCluster This workshop shows advanced techniques with Slurm on ParallelCluster, including federation, accounting, and using the Slurm REST API with Jupyter notebooks   Setting up a client library for the Slurm REST API A Jupyter notebook walkthrough of building and using a Python client library for the Slurm REST API    "}]