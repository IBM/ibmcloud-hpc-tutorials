[{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/04-custom-image/01-create-vsi-from-default-lsf-image/","title":"Create VSI From Default LSF Image","tags":[],"description":"","content":"Start IBM Cloud Shell from the browser Find the icon to lauch Cloud Shell in the dash board:\nIt will create another browser window with the Cloud Shell:\nIn the Cloud Shell, follow the next steps to create a VSI (assuming bash):\n Assign values to environment variables for commands later  # Replace CLUSTER_PREFIX with the one you used to create the existing LSF cluster # Replace KEY_NAME, RESOURCE_GROUP_NAME, ZONE_NAME and CLUSTER_IMG_ID as needed CLUSTER_PREFIX=\u0026lt;cluster-name-prefix\u0026gt; CLUSTER_VPC_NAME=${CLUSTER_PREFIX}-vpc CLUSTER_SUBNET_NAME=${CLUSTER_PREFIX}-subnet CLUSTER_IMG_ID=r006-68478a2e-4abc-4bfb-9e4f-a6fb3b9b235f CLUSTER_SG_NAME=${CLUSTER_PREFIX}-sg CUSTOM_IMG_NAME=${CLUSTER_PREFIX}-custom-img CUSTOM_IMG_VSI_NAME=${CUSTOM_IMG_NAME}-vsi ZONE_NAME=us-south-3 PROFILE_NAME=bx2-2x8 KEY_NAME=ccyang-pub-key RESOURCE_GROUP_NAME=Default The CLUSTER_IMG_ID can be found from the open-source terraform script in IBM Cloud hpc-cluster-lsf github repository. You can choose the OS you wish to use. You should make sure the region key is the same as where you will create the VSI and where the custom image should be.\nCreate a VSI  ibmcloud is instance-create \\ ${CUSTOM_IMG_VSI_NAME} \\ ${CLUSTER_VPC_NAME} \\ ${ZONE_NAME} \\ ${PROFILE_NAME} \\ ${CLUSTER_SUBNET_NAME} \\ --image ${CLUSTER_IMG_ID} \\ --keys ${KEY_NAME} \\ --resource-group-name ${RESOURCE_GROUP_NAME} \\ --sgs ${CLUSTER_SG_NAME} Use the following command to check the status of the new VSI. Wait till the instance is in running state.  ibmcloud is instance ${CUSTOM_IMG_VSI_NAME} Get assigned IP address  ibmcloud is instance ${CUSTOM_IMG_VSI_NAME} --output JSON | jq -r .network_interfaces[0].primary_ipv4_address Keep the Cloud Shell open, we will reuse it in step 03 to create the custom image.\n"},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/03-bursting/01-vpn/","title":"Set up VPN","tags":["burst"],"description":"","content":"The automation package is implemented as a Terraform template for VPCs on IBM Cloud. Users can configure and start their cluster creation with Schematics, a Terraform platform with Web UI on IBM Cloud. The template is publicly available at GitHub for IBM Cloud. All you need to create a cloud HPC cluster is to set the GitHub link to a Schematics workspace, edit pre-defined variables, and click several buttons. This tutorial gives a step-by-step guide to creating an example VPN deployment.\nBefore you begin, make sure to complete the first four steps for getting started with IBM Spectrum LSF. Also, you need to know the public IP address of your local VPN server, a local CIDR accessing the VPN environment, and a preshared key to authenticate your VPN connection. The preshared key can be any random string.\nThe following figure is an example of a VPN deployment.\n This example links a VPN gateway to the subnet for LSF nodes. By doing this, local clients can directly access them with private IP addresses (for example, 192.168.3.21 to 10.248.0.37).\n  Configure VPN deployment variables\nSet VPN deployment variables when you create your workspace. In addition to essential variables to construct your cluster (e.g., api_key), you need to set vpn_enabled to be true, and then, specify vpn_peer_address, vpn_peer_cidrs, and vpn_preshared_key to be identical to the public IP address for your local VPN server, a local CIDR accessing to the VPN environment, and a preshared key, respectively. In the case of the above example architecture, set vpn_peer_address to be 60.150.xxx.yyy, and vpn_peer_cidrs to be 192.168.3.0/24.\n  Apply plan\nApply a plan to build your cluster with a VPN gateway. After a while, Schematics logs show you essential information to configure your local VPN environment. You can use the Web console to check the log files. They show a line vpn_config_info =..., which contains the VPN public IP (162.133.aaa.bbb), the connected CIDR (10.248.0.32/27), and used UDP ports.\n2021/10/07 04:06:48 Terraform apply | Outputs: 2021/10/07 04:06:48 Terraform apply | 2021/10/07 04:06:48 Terraform apply | ssh_command = \u0026#34;ssh -J root@163.68.xxx.yyy lsfadmin@10.248.0.37\u0026#34; 2021/10/07 04:06:48 Terraform apply | vpc_name = \u0026#34;lsf-cloud-vpc -- - r034-3cdad0f9-42b6-4ce3-b1c5-61c9f3b55cfa\u0026#34; 2021/10/07 04:06:48 Terraform apply | vpn_config_info = \u0026#34;IP: 163.68.aaa.bbb, CIDR: 10.248.0.32/27, UDP ports: 500, 4500\u0026#34; 2021/10/07 04:06:48 Command finished successfully. 2021/10/07 04:06:53 Done with the workspace action   Configure your local VPN environment\nTypical VPN configurations require a public IP address for the local VPN server, a local CIDR, a preshared key, a peer IP address, and a peer CIDR. In the example above, you first need to configure your local VPN server with public IP address, local CIDR, and a preshared key, which are identical to what you specified for vpn_peer_address, vpn_peer_cidr, and vpn_preshared_key at Step 1, respectively. Then, your local VPN configuration needs to add a peer IP address to be 163.68.aaa.bbb and a peer CIDR to be 10.248.0.32/27 according to the output of Step 2. Finally, UDP ports 500 and 4500 must be accessible from the VPN gateway on IBM Cloud by configuring your local network devices (e.g., routers). For more details on configuring your VPN, see Connecting to your on-premises network.\n  "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/01-hpc-overview/","title":"HPC on Cloud Overview","tags":[],"description":"","content":"High Performance Computing (HPC) in the IBM Cloud can accelerate results by scaling a vast number of tasks and reduce costs by optimizing the right technology to meet your specifc goals. The cloud is always ready whenever you are ready to scale up. Whether you need a single CPU core or tens of thousands cores, a traditional MPI-oriented cluster or fully containerized Openshift cluster, IBM Cloud has the solution to meet the needs.\nFor traditional MPI-centric HPC workloads, the most natural way to utilize cloud resources is the Virtual Private Cloud (VPC). Essentially, it provides a set of configurable resources for building a secure cluster on a public cloud. In its most essential form, a virtual private cloud consists of a number of virtual machine instances, connected through a software-define network which is isolated from other VPCs on the same public cloud. VPCs are a \u0026ldquo;best of both worlds\u0026rdquo; approach to cloud computing. They give customers many of the advantages of private clouds, while leveraging public cloud resources and savings.\n   Component Traditional HPC cluster VPC cluster     Unit of compute node A bare-metal node, fixed configuration A virtual machine instance (VSI), flexible   Interconnect Physical Ethernet, Infiniband, etc. Software defined network    "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/04-custom-image/02-customize-software-on-the-vsi/","title":"Customize Software on the VSI","tags":[],"description":"","content":"From your local machine, login to the newly created VSI by running the following command (Note: After the VSI is in running state, you might still need to wait a few minutes before you can connect to it via ssh).\nssh -J root@\u0026lt;jump-node-ip\u0026gt; root@\u0026lt;new-instance-ip\u0026gt; You should find the jump-node-ip from the ssh login command generated while creating the existing LSF cluster.\nNow customize the software on the VSI boot volume.\n"},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/03-bursting/02-setup/","title":"Set up multi-cluster and job forwarding","tags":["burst"],"description":"","content":"et up the LSF multi-cluster and job forwarding. This tutorial assumes that the small on-premise cluster labeled with \u0026ldquo;OnPremiseCluster\u0026rdquo; uses a subnet 192.168.3.0/24. The cluster runs a master node at 192.168.3.21 (on-premise-master) and two single-CPU workers (i.e., up to two CPUs are available for jobs). The cloud cluster labeled with \u0026ldquo;HPCCluster\u0026rdquo; uses a subnet 10.248.0.32/27 and its master uses 10.248.0.37 (icgen2host-10-248-0-37) without any static workers created. Both of the configuration directories are in /opt/ibm/lsf/conf, but you can change the directory depending on your cluster configuration.\n  The following is an example of the /etc/hosts file for the cloud cluster. You need to make sure that the hostnames for the LSF masters are DNS-resolvable.\n... 10.248.0.61 icgen2host-10-248-0-61 10.248.0.62 icgen2host-10-248-0-62 10.248.0.63 icgen2host-10-248-0-63 192.168.3.21 on-premise-master # added For the on-premises /etc/hosts file, make sure to add the information about the master node in the cloud cluster:\n192.168.3.21 on-premise-master 192.168.3.22 on-premise-worker-0 192.168.3.23 on-premise-worker-1 10.248.0.37 icgen2host-10-248-0-37 #added   Both clusters need to recognize each other, so you need to modify /opt/ibm/lsf/conf/lsf.shared. This configuration file should be identical in both clusters.\n... Begin Cluster ClusterName Servers # Keyword # modified HPCCluster (icgen2host-10-248-0-37) # modified OnPremiseCluster (on-premise-master) # modified End Cluster ...   The two clusters are configured to have different lsb.queues files. In the cloud cluster, you need to append the following lines to /opt/ibm/lsf/conf/lsbatch/HPCCluster/configdir/lsb.queues to register a receive queue:\n... Begin Queue QUEUE_NAME=recv_q RCVJOBS_FROM=OnPremiseCluster PRIORITY=30 NICE=20 RC_HOSTS=all End Queue The on-premises cluster is configured to have a send queue at /opt/ibm/lsf/conf/lsbatch/OnPremiseCluster/configdir/lsb.queues:\n... Begin Queue QUEUE_NAME=send_q SNDJOBS_TO=recv_q@allclusters PRIORITY=30 NICE=20 End Queue   Restart both clusters by running the following command:\n$ lsfrestart   Check if two clusters are connected\n$ lsclusters -w CLUSTER_NAME STATUS MASTER_HOST ADMIN HOSTS SERVERS OnPremiseCluster ok on-premise-master lsfadmin 3 3 HPCCluster ok icgen2host-10-248-0-37 lsfadmin 1 1   "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/02-setup-lsf-cluster/","title":"Standing up an LSF Cluster","tags":["HPC","Introduction","LSF","Optional"],"description":"","content":"IBM Spectrum LSF is part of the HPC Clustering Services on IBM Cloud to enable users to stand up a HPC environment within minutes. LSF is an advanced workload management solution for demanding HPC environments. HPC cluster deployment starts with the definition of the desired cluster configuration; for example, count of management nodes, worker nodes, amount of storage and so on. Open-source technologies like Terraform and Ansible have been used to automate the provisioning and configuration of all associated IBM Cloud VPC resources and LSF.\nIn this section, you are using the IBM Spectrum LSF catalog tile to create a HPC cluster in IBM Cloud and run a LSF job as you would on-premises. This includes the following steps:\n Prepare your SSH key and IBM Cloud API keys. Configure cluster parameters from your IBM Cloud web console. Create your first cluster. Submit a sample job and check what is happening in the background. Delete the cluster.  "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/04-custom-image/03-create-image-from-customized-volume/","title":"Create Image From Customized Volume","tags":[],"description":"","content":"Now, we go back to the Cloud Shell for creating the new custom image.\n Stop the VSI  ibmcloud is instance-stop ${CUSTOM_IMG_VSI_NAME} Submit the request to create the custom image  # Store Volume ID in a environment variable SOURCE_VOLUME_ID=$(ibmcloud is instance ${CUSTOM_IMG_VSI_NAME} --output JSON | jq -r .volume_attachments[0].volume.id) # Submit request to create image from the volume ibmcloud is image-create ${CUSTOM_IMG_NAME} --source-volume ${SOURCE_VOLUME_ID} This step can take some time depending on the image size. You can check the webpage of the images (Navigation Menu \u0026ndash;\u0026gt; VPC Infrastructure \u0026ndash;\u0026gt; Custom images) for the status of the image creation.\nWhen the image is created, we can delete the VSI that is no longer needed  ibmcloud is instance-delete ${CUSTOM_IMG_VSI_NAME} "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/03-bursting/03-tryout/","title":"Try cloud bursting","tags":["burst"],"description":"","content":"Now you can forward jobs from on-premises to the cloud. This tutorial tries to request three CPUs by starting three concurrent sleep 10s although the example on-premise cluster has only two CPUs. At your on-premises cluster, you can test the following command:\n$ for i in `seq 1 3`; do bsub -q send_q sleep 10s; done The cloud cluster needs a few minutes to allocate a dynamic host. After a while, you can check if cloud bursting works at on-premise-master.\n$ bjobs -aw JOBID USER STAT QUEUE FROM_HOST EXEC_HOST JOB_NAME SUBMIT_TIME 1 lsfadmin DONE send_q on-premise-master on-premise-worker-0 sleep 10s Oct 7 01:02 2 lsfadmin DONE send_q on-premise-master on-premise-worker-1 sleep 10s Oct 7 01:02 3 lsfadmin DONE send_q on-premise-master icgen2host-10-248-0-38@HPCCluster sleep 10s Oct 7 01:02 You can also see that the job comes to a node at HPCCluster.\n$ bjobs -aw JOBID USER STAT QUEUE FROM_HOST EXEC_HOST JOB_NAME SUBMIT_TIME 101 lsfadmin DONE recv_q on-premise-master@OnPremiseCluster:3 icgen2host-10-248-0-38 sleep 10s Oct 7 01:02 You can see icgen2host-10-248-0-38 has been created. If you do not start any jobs for a while, the host is automatically released (marked as closed_RC).\n$ bhosts -aw HOST_NAME STATUS JL/U MAX NJOBS RUN SSUSP USUSP RSV icgen2host-10-248-0-37 closed_Full - 0 0 0 0 0 0 icgen2host-10-248-0-38 closed_RC - 1 0 0 0 0 0 "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/03-bursting/","title":"Cloud Bursting","tags":["HPC","Introduction","LSF","Optional"],"description":"","content":"IBM Cloud offers virtual private clouds (VPCs), which enable elastic provisioning of x86 virtual servers with the pay-as-you-go model. Users can optimize both the cost and performance of their target workloads by maintaining the number of virtual servers and their resource configurations. VPCs support various instance flavors of virtual machines with from 1 to 128 virtual CPUs, from 4GB to 1TB RAM, and various capacity and performance settings of storage and networking.\nIn this post, we focus on an automation package for Spectrum LSF.\nIBM Spectrum LSF is widely used to manage parallel distributed HPC workloads. The automation package deploys LSF management and compute hosts with a shared filesystem. It also configures auto-scaling of compute hosts with its resource connector for IBM Cloud. The deployed cluster runs the Red Hat Enterprise Linux 7.7 with pre-installed software packages such as OpenMPI.\n The automation package enables users to quickly create an HPC cluster on IBM Cloud. Users can import data to the shared filesystem and submit HPC jobs using the bsub command. The automation can be regarded as a quick starter kit for HPC in the cloud, but also it enables cloud bursting, a technique to deal with the temporal insufficiency of computing resources at an on-premise cluster using the elastic cloud infrastructures.\nLSF can support cloud bursting using multi-cluster setups and job forwarding. With these features, LSF can forward jobs from on-premise to a cloud cluster when the on-premise cannot satisfy requests for computing resources. On the other hand, the cloud cluster can be scaled down to save the costs for compute nodes during idle time.\nThe automation package also supports the configuration of a virtual private network (VPN) for an LSF cluster. VPN enables secure network connections between on-premise and cloud clusters as if they were directly connected. On top of the VPN connectivity, two LSF clusters can securely share cluster information and forward jobs over the internet.\nThis tutorial consists of three parts:\n Setting up VPN Setting up multi-cluster and job forwarding Trying simple workloads with cloud bursting.  "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/04-custom-image/04-create-cluster-from-custom-image/","title":"Create Cluster From Custom Image","tags":[],"description":"","content":"The last step is to use the newly created image to create a new LSF cluster. We are working on adding the description for how to do so and we will add it very soon.\n"},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/04-custom-image/","title":"Custom Image (Under Construction)","tags":["HPC","LSF","Optional"],"description":"","content":"In this tutorial, we will guide you through the steps to create a custom image that allows you to install custom software packages and later reuse the image for creating new LSF clusters. We assume you have followed the steps in II - Standing up an LSF Cluster to create an existing LSF cluster, and we will reuse its VPC infrastructure to create a VSI within it.\n"},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/02-setup-lsf-cluster/01-sshkey-api/","title":"Prepare SSH and API Keys","tags":["LSF"],"description":"","content":"Create Your SSH KEY After you sign in to https://cloud.ibm.com/ with your account, start with the Navigation Menu on the dashboard in the top left corner of the page:\n  Click VPC Infrastructure \u0026gt; SSH Keys\n  Click Create\n   Add name (e.g. demo-ssh-key), type your resource group name and select region Copy the public key and paste it in the public_key field(e.g.: contents in .ssh/id_rsa.pub)  Click on Add SSH Key  If you do not have SSH keys on your machine where you plan to log in to your HPC cluster, use the ssh-keygen command to generate.\n[root@hf-ocp-login1 ~]# ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub. Copy and paste the content in /root/.ssh/id_rsa.pub to the public key field in Step 2.\nPlease remember your ssh key name (e.g., demo-ssh-key), the resource group name and the region you have selected. You will need to use those in the next step when creating a cluster.\nCreate Your IBM Cloud API Key   Go to Manage \u0026gt; Access (IAM)\n  Click API keys\n  Click Create an IBM Cloud API key\n3.1. Enter a name for your API key and Click Create\n3.2. Then, click Show to display the API key. Or, click Copy to copy and save it for later, or click Download   Please keep the API key saved in Step 3.2 in a secure place. You will need to use this in the next step when creating the cluster.\n"},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/02-setup-lsf-cluster/02-hpc-cluster-catalog/","title":"Create Your Cluster using IBM Spectrum LSF","tags":["LSF"],"description":"","content":"After you sign in to https://cloud.ibm.com/ with your account, search \u0026ldquo;HPC\u0026rdquo; or \u0026ldquo;Spectrum LSF\u0026rdquo; in IBM Cloud catalog. Find \u0026ldquo;IBM Spectrum LSF\u0026rdquo; and select the service. It will lead you to the HPC Cluster solution page. Or the tile can be directly accessed from here.\nScroll down to the Configure your workspace section. The resource group parameter in this section is where the Schematics workspace is provisioned on your IBM Cloud account. The value for this parameter can be different than the one used for SSH key. You can leave the other fields with the default.\nScroll down to the Set the deployment values section. You will need to fill out three fields in this Parameters without default values section. You can toggle Use an existing secret to No for the api_key parameter and paste your api key saved earlier. Enter \u0026ldquo;true\u0026rdquo; for lsf_license_confirmatin and your ssh_key_name (e.g.: demo-ssh-key).\nContinue with the Parameters with default values section. Though you can leave most of the parameters with default values, you need to modify a few parameters depending on your cloud account and ssh key.\n Please select a cluster_prefix string that would be unique to you (e.g.: use your initial here). The resource group value needs to be consistent with the one which you use to create your SSH key. The region (may not be required in the future) and zone need to match where your ssh key is created.  a list of supported regions: au-syd,br-sao,ca-tor,eu-de,eu-gb,jp-osa,jp-tok,us-east,us-south each region has 3 zones (e.g.: us-east-1, us-east-2, us-east-3 for the us-east region)   Please just use 1 for management_node_count.  To learn more about cluster configuration parameters, please refer to this tutorial in Step 4.\nNow you are ready to create the cluster. In the bottom right corner of the page, select the license agreements button and click Install. It will lead to your Schematics workspace and will take few minutes for your cluster to be ready. "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/02-setup-lsf-cluster/03-access-cluster/","title":"Access Your Cluster","tags":["LSF"],"description":"","content":"Overview of the HPC Cluster The HPC Cluster consists of a login node, a storage node where the block storage volume is being attached to, 1 to 3 LSF management nodes, and a number of LSF worker nodes.\n   The login node is served as a jump host and it is the only node which has the public IP address. Other nodes would only have private IP addresses and the only way to reach to these nodes is through the login node. Users should log in to the primary LSF management node (or LSF master) and do most of the operations from the LSF master. By default, lsfadmin is the only user id being created on the cluster. The ssh passwordless setup is configured between the LSF master and workers. Users can reach to any other worker node with the lsfadmin user id from the LSF master.\n  The worker node can be a static resource. In this case its lifecycle is managed by Schematics/Terraform. Users can request a number of static worker nodes and these workers remain available in the LSF cluster until a Schematics/Terraform destroy action is being performed. The LSF Resource Connector functionality creates additional workers when there is not enough capacity to run jobs and destroys workers when the demands decrease. The lifecycle of these dynamic workers is managed by the LSF Resource Connector. Users should wait these dynamic resources returned to the Cloud before destroying the entire VPC cluster through Schematics/Terraform.\n  The storage node is configured as an NFS server and the block storage volume is mounted to /data which is exported to share with LSF cluster nodes. At the NSF client end, the LSF cluster nodes in this case, we mount the remote directory, /data, to /mnt/data locally. A soft link, /home/lsfadmin/shared, also points to /mnt/data. Users can use /home/lsfadmin/shared as a shared file system for their applications.\n  Schematics Workspace After you click Install, you will be taken to a Schematics workspace created in your Cloud account show as below. This is where you manage your HPC cluster. You can learn more about IBM Cloud Schematics Solution here.\nClick on Jobs and you will be updated with the progress of cluster creation. Once it\u0026rsquo;s completed, you will be given a ssh command towards the end of this log file. Go to the terminal where you have your SSH key and run the ssh command. You can use lsid to display the LSF cluster information.\n% ssh -J root@141.125.159.202 lsfadmin@10.242.64.37 Last login: Mon Dec 6 14:38:05 2021 from 10.242.64.4 [lsfadmin@icgen2host-10-242-64-37 ~]$ lsid IBM Spectrum LSF Standard 10.1.0.11, Jul 16 2021 Copyright International Business Machines Corp. 1992, 2016. US Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp. My cluster name is HPCCluster My master name is icgen2host-10-242-64-37 That\u0026rsquo;s it. You are on the LSF management node and the cluster is ready for you to run your HPC workloads.\n"},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/02-setup-lsf-cluster/04-run-job/","title":"Run LSF Jobs Using Auto-Scaling","tags":["LSF"],"description":"","content":"worker_node_max_count and _worker_node_min_count are the parameters for you to configure auto-scaling. If you leave worker_node_max_count and _worker_node_min_count parameters with default values, you currently would have no worker node in your cluster.\nIn the example below, icgen2host-10-242-64-37 is the only node in your LSF cluser and is served as the LSF management node which is configured not to run users\u0026rsquo; workloads.\n[lsfadmin@icgen2host-10-242-64-37 ~]$ bhosts -w HOST_NAME STATUS JL/U MAX NJOBS RUN SSUSP USUSP RSV icgen2host-10-242-64-37 closed_Full - 0 0 0 0 0 0 In addition, since the worker_node_max_count parameter is set to 10, you can only submit jobs that use no more than 10 nodes. Here, we submit a simple sleep job by requesting 2 worker nodes and observe how auto-scaling works behind the scene to dynamically provision two nodes and add to the existing cluster.\n[lsfadmin@icgen2host-10-242-64-37 ~]$ bsub -n 2 -R \u0026#34;span[ptile=1]\u0026#34; sleep 10 Job \u0026lt;3\u0026gt; is submitted to default queue \u0026lt;normal\u0026gt;. Wait for few minutes. Run bhosts again and you will see two new worker nodes added to the cluster by auto-scaling.\n[lsfadmin@icgen2host-10-242-64-37 ~]$ bhosts -w HOST_NAME STATUS JL/U MAX NJOBS RUN SSUSP USUSP RSV icgen2host-10-242-64-37 closed_Full - 0 0 0 0 0 0 icgen2host-10-242-64-38 ok - 4 0 0 0 0 0 icgen2host-10-242-64-40 ok - 4 0 0 0 0 0 The submitted job is shown as completed running the sleep command on these two new nodes.\n[lsfadmin@icgen2host-10-242-64-37 ~]$ bjobs -a -w JOBID USER STAT QUEUE FROM_HOST EXEC_HOST JOB_NAME SUBMIT_TIME 3 lsfadmin DONE normal icgen2host-10-242-64-37 icgen2host-10-242-64-38:icgen2host-10-242-64-40 sleep 10 Dec 7 11:55 icgen2host-10-242-64-40 After the new nodes have been idling for 10 minutes, auto-scaling will destroy the resources and have those removed from your cluster.\nNow let\u0026rsquo;s run a simple MPI job. Go to /home/lsfadmin/shared and this is the directory which is visible to all nodes in your cluster. Threfore, you should place your MPI workloads under this directory. OpenMPI 4.1 is already installed under /usr/local/openmpi-4.1.0 and you can include its bin/ directory to your PATH environment variable.\nlsfadmin@icgen2host-10-242-64-37:~\u0026gt;cd shared/ lsfadmin@icgen2host-10-242-64-37:~/shared\u0026gt;pwd /home/lsfadmin/shared lsfadmin@icgen2host-10-242-64-37:~/shared\u0026gt; export PATH=/usr/local/openmpi-4.1.0/bin:$PATH lsfadmin@icgen2host-10-242-64-37:~/shared\u0026gt;which mpicc /usr/local/openmpi-4.1.0/bin/mpicc Let\u0026rsquo;s create a MPI program named hello.c and build it using mpicc from OpenMPI 4.1.\nlsfadmin@icgen2host-10-242-64-37:~/shared\u0026gt;cat \u0026gt; hello.c \u0026lt;\u0026lt; EOF \u0026gt; #include \u0026lt;mpi.h\u0026gt; \u0026gt; #include \u0026lt;stdio.h\u0026gt; \u0026gt; #include \u0026lt;stdlib.h\u0026gt; \u0026gt; #include \u0026lt;unistd.h\u0026gt; \u0026gt; \u0026gt; int main(int argc, char *argv[]) \u0026gt; { \u0026gt; int myrank; \u0026gt; char hostname[50]; \u0026gt; \u0026gt; gethostname(hostname, 50); \u0026gt; printf(\u0026#34;Hello from %s (before MPI_Init)...\\n\u0026#34;, hostname); \u0026gt; \u0026gt; MPI_Init(\u0026amp;argc, \u0026amp;argv); \u0026gt; //sleep(600); \u0026gt; MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;myrank); \u0026gt; printf(\u0026#34;Hello from %s, rank %d...\\n\u0026#34;, hostname, myrank); \u0026gt; \u0026gt; MPI_Finalize(); \u0026gt; \u0026gt; return 0; \u0026gt; } \u0026gt; EOF lsfadmin@icgen2host-10-242-64-37:~/shared\u0026gt;mpicc -g -o hello hello.c We now run a MPI job with 4 MPI tasks on two nodes, each node with 2 MPI ranks.\nlsfadmin@icgen2host-10-242-64-37:~/shared\u0026gt;bsub -o %J.out -e %J.err -n 4 -R \u0026#34;span[ptile=2]\u0026#34; mpirun ./hello Job \u0026lt;5\u0026gt; is submitted to default queue \u0026lt;normal\u0026gt;. Check the 5.out file:\nlsfadmin@icgen2host-10-242-64-37:~/shared\u0026gt;cat 5.out Sender: LSF System \u0026lt;lsfadmin@icgen2host-10-242-64-38\u0026gt; Subject: Job 5: \u0026lt;mpirun ./hello\u0026gt; in cluster \u0026lt;HPCCluster\u0026gt; Done Job \u0026lt;mpirun ./hello\u0026gt; was submitted from host \u0026lt;icgen2host-10-242-64-37\u0026gt; by user \u0026lt;lsfadmin\u0026gt; in cluster \u0026lt;HPCCluster\u0026gt; at Tue Dec 7 13:43:30 2021 Job was executed on host(s) \u0026lt;2*icgen2host-10-242-64-38\u0026gt;, in queue \u0026lt;normal\u0026gt;, as user \u0026lt;lsfadmin\u0026gt; in cluster \u0026lt;HPCCluster\u0026gt; at Tue Dec 7 13:45:28 2021 \u0026lt;2*icgen2host-10-242-64-43\u0026gt; \u0026lt;/home/lsfadmin\u0026gt; was used as the home directory. \u0026lt;/home/lsfadmin/shared\u0026gt; was used as the working directory. Started at Tue Dec 7 13:45:28 2021 Terminated at Tue Dec 7 13:45:30 2021 Results reported at Tue Dec 7 13:45:30 2021 Your job looked like: ------------------------------------------------------------ # LSBATCH: User input mpirun ./hello ------------------------------------------------------------ Successfully completed. Resource usage summary: CPU time : 0.27 sec. Max Memory : - Average Memory : - Total Requested Memory : - Delta Memory : - Max Swap : - Max Processes : - Max Threads : - Run time : 4 sec. Turnaround time : 120 sec. The output (if any) follows: Hello from icgen2host-10-242-64-38 (before MPI_Init)... Hello from icgen2host-10-242-64-38 (before MPI_Init)... Hello from icgen2host-10-242-64-43 (before MPI_Init)... Hello from icgen2host-10-242-64-43 (before MPI_Init)... Hello from icgen2host-10-242-64-38, rank 0... Hello from icgen2host-10-242-64-38, rank 1... Hello from icgen2host-10-242-64-43, rank 2... Hello from icgen2host-10-242-64-43, rank 3... PS: Read file \u0026lt;5.err\u0026gt; for stderr output of this job. More information about LSF You can learn more about LSF here.\nBelow is a cheat sheet for useful LSF commands: "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/02-setup-lsf-cluster/05-manage-cluster/","title":"Manage your Cluster","tags":["LSF"],"description":"","content":"You can manage your VPC resources in the Schematics workspace. You can use Resources in the menu on the left to visualize all the resources created for your HPC cluster. The cluster configuration parameters will be listed when you click Settings. When you are ready to destroy your cluster, you can use Actions \u0026gt; Destroy resources to destroy your cluster. If you no longer want to keep the workspace, you can select Delete workspace instead. It has options to allow you to both destroy your cluster as well as your workspace.\n"},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/tags/hpc/","title":"HPC","tags":[],"description":"","content":""},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/tags/lsf/","title":"LSF","tags":[],"description":"","content":""},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/tags/optional/","title":"Optional","tags":[],"description":"","content":""},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/tags/","title":"Tags","tags":[],"description":"","content":""},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/tags/burst/","title":"burst","tags":[],"description":"","content":""},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/tags/introduction/","title":"Introduction","tags":[],"description":"","content":""},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/","title":"HPC on IBM Cloud Tutorials","tags":[],"description":"","content":"Welcome to HPC on IBM Cloud Tutorials This website is a growing open-source documentation project for \u0026ldquo;HPC on IBM Cloud\u0026rdquo; topics. We aim to provide tailored, step-by-step tutorials on various tasks in utilizing public cloud resources for HPC workloads. We hope to smooth the learning curve for the transition from traditional on-prem HPC clusters to cloud-based or even cloud-native workflows. Most information here are derived from references and tutorials from IBM Cloud\u0026rsquo;s online articles.\nCurrent topics:\n  Cloud Basics introduces the building blocks of cloud computing and basic preparations for going through the other topics.\n  Standing up an LSF Cluster shows how to set up a HPC cluster on IBM Cloud, including how to use the autoscaling capability.\n  Hybrid Cloud Computing explores the exciting possiblities of leveraging hybrid cloud for HPC\n  HPC File Systems discusses the options for a high-performance parallel file system\n  Select a topic from the nagivation panel on the left or continue to HPC on Cloud Overview.\n"},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/conferences/","title":"Conferences Workshops","tags":[],"description":"","content":"List of workshops given at confernces    Name Description     reInvent 2021 AWS ParallelClusterAPI Introduction to the AWS ParallelCluster API and the PCluster Manager web UI to manage clusters   Supercomputing 2021 Tutorial Collection of labs created for the full-day tutorial delivered by the AWS HPC team at SC21   Supercomputing 2020 Tutorial Find the labs created for the full-day tutorial delivered by the AWS HPC team at SC20   Supercomputing 2019 Tutorial Collection of labs created for the first full-day tutorial delivered by the AWS HPC team at SC19    "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/authors/","title":"Credits","tags":[],"description":"","content":" I-hsin Chung Ming Hung Chen Constantinos Evangelinos Lixiang Luo Dale Pearson Seetharami Seelam Robert Walkup Hui-fang Wen Chih Chieh Yang  "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/external/","title":"External Workshops","tags":[],"description":"","content":"List of External workshops    Name Description     Advanced Slurm on ParallelCluster This workshop shows advanced techniques with Slurm on ParallelCluster, including federation, accounting, and using the Slurm REST API with Jupyter notebooks   Setting up a client library for the Slurm REST API A Jupyter notebook walkthrough of building and using a Python client library for the Slurm REST API    "}]