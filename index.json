[{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/04-custom-image/01-create-vsi-from-default-lsf-image/","title":"Create VSI From Default LSF Image","tags":[],"description":"","content":"Start IBM Cloud Shell from the browser (Insert image here)\nIn the Cloud Shell, follow the next steps to create a VSI (assuming bash):\n Assign values to environment variables for commands later  # Replace CLUSTER_PREFIX with the one you used to create the existing LSF cluster # Replace KEY_NAME, RESOURCE_GROUP_NAME, ZONE_NAME and CLUSTER_IMG_ID as needed CLUSTER_PREFIX=\u0026lt;cluster-name-prefix\u0026gt; CLUSTER_VPC_NAME=${CLUSTER_PREFIX}-vpc CLUSTER_SUBNET_NAME=${CLUSTER_PREFIX}-subnet CLUSTER_IMG_ID=r006-68478a2e-4abc-4bfb-9e4f-a6fb3b9b235f CLUSTER_SG_NAME=${CLUSTER_PREFIX}-sg CUSTOM_IMG_NAME=${CLUSTER_PREFIX}-custom-img CUSTOM_IMG_VSI_NAME=${CUSTOM_IMG_NAME}-vsi ZONE_NAME=us-south-3 PROFILE_NAME=bx2-2x8 KEY_NAME=ccyang-pub-key RESOURCE_GROUP_NAME=Default (explain how to get the CLUSTER_IMG_ID)\nCreate a VSI  ibmcloud is instance-create \\ ${CUSTOM_IMG_VSI_NAME} \\ ${CLUSTER_VPC_NAME} \\ ${ZONE_NAME} \\ ${PROFILE_NAME} \\ ${CLUSTER_SUBNET_NAME} \\ --image ${CLUSTER_IMG_ID} \\ --keys ${KEY_NAME} \\ --resource-group-name ${RESOURCE_GROUP_NAME} \\ --sgs ${CLUSTER_SG_NAME} Use the following command to check the status of the new VSI. Wait till the instance is in running state.  ibmcloud is instance ${CUSTOM_IMG_VSI_NAME} Get assigned IP address  ibmcloud is instance ${CUSTOM_IMG_VSI_NAME} --output JSON | jq -r .network_interfaces[0].primary_ipv4_address Keep the Cloud Shell open, we will reuse it in step 03 to create the custom image.\n"},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/03-bursting/01-vpn/","title":"Set up VPN","tags":["burst"],"description":"","content":"The automation package is implemented as a Terraform template for VPCs on IBM Cloud. Users can configure and start their cluster creation with Schematics, a Terraform platform with Web UI on IBM Cloud. The template is publicly available at GitHub for IBM Cloud. All you need to create a cloud HPC cluster is to set the GitHub link to a Schematics workspace, edit pre-defined variables, and click several buttons. This tutorial gives a step-by-step guide to creating an example VPN deployment.\nBefore you begin, make sure to complete the first four steps for getting started with IBM Spectrum LSF. Also, you need to know the public IP address of your local VPN server, a local CIDR accessing the VPN environment, and a preshared key to authenticate your VPN connection. The preshared key can be any random string.\nThe following figure is an example of a VPN deployment.\nThis example links a VPN gateway to the subnet for LSF nodes. By doing this, local clients can directly access them with private IP addresses (for example, 192.168.3.21 to 10.248.0.37).\n  Configure VPN deployment variables\nSet VPN deployment variables when you create your workspace. In addition to essential variables to construct your cluster (e.g., api_key), you need to set vpn_enabled to be true, and then, specify vpn_peer_address, vpn_peer_cidrs, and vpn_preshared_key to be identical to the public IP address for your local VPN server, a local CIDR accessing to the VPN environment, and a preshared key, respectively. In the case of the above example architecture, set vpn_peer_address to be 60.150.xxx.yyy, and vpn_peer_cidrs to be 192.168.3.0/24.\n  Apply plan\nApply a plan to build your cluster with a VPN gateway. After a while, Schematics logs show you essential information to configure your local VPN environment. You can use the Web console to check the log files. They show a line vpn_config_info =..., which contains the VPN public IP (162.133.aaa.bbb), the connected CIDR (10.248.0.32/27), and used UDP ports.\n2021/10/07 04:06:48 Terraform apply | Outputs: 2021/10/07 04:06:48 Terraform apply | 2021/10/07 04:06:48 Terraform apply | ssh_command = \u0026#34;ssh -J root@163.68.xxx.yyy lsfadmin@10.248.0.37\u0026#34; 2021/10/07 04:06:48 Terraform apply | vpc_name = \u0026#34;lsf-cloud-vpc -- - r034-3cdad0f9-42b6-4ce3-b1c5-61c9f3b55cfa\u0026#34; 2021/10/07 04:06:48 Terraform apply | vpn_config_info = \u0026#34;IP: 163.68.aaa.bbb, CIDR: 10.248.0.32/27, UDP ports: 500, 4500\u0026#34; 2021/10/07 04:06:48 Command finished successfully. 2021/10/07 04:06:53 Done with the workspace action   Configure your local VPN environment\nTypical VPN configurations require a public IP address for the local VPN server, a local CIDR, a preshared key, a peer IP address, and a peer CIDR. In the example above, you first need to configure your local VPN server with public IP address, local CIDR, and a preshared key, which are identical to what you specified for vpn_peer_address, vpn_peer_cidr, and vpn_preshared_key at Step 1, respectively. Then, your local VPN configuration needs to add a peer IP address to be 163.68.aaa.bbb and a peer CIDR to be 10.248.0.32/27 according to the output of Step 2. Finally, UDP ports 500 and 4500 must be accessible from the VPN gateway on IBM Cloud by configuring your local network devices (e.g., routers). For more details on configuring your VPN, see Connecting to your on-premises network.\n  "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/01-hpc-overview/","title":"HPC on Cloud Overview","tags":[],"description":"","content":"Current topics === TO EXPAND ===:\n  Cloud Basics introduces the basic building blocks of cloud computing\n  Standing up an LSF Cluster shows how to set up a HPC cluster on IBM Cloud\n  HPC File Systems discusses the options for a high-performance parallel file system\n  Autoscaling with LAMMPS demonstrates the autoscaling capability using LAMMPS as the example workload.\n  Hybrid Cloud Computing explores the exciting possiblities of leveraging hybrid cloud for HPC\n  We recommend you take these labs in the order presented as some dependencies exists between them, but feel free to change the order based on your comfort level.\n"},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/04-custom-image/02-customize-software-on-the-vsi/","title":"Customize Software on the VSI","tags":[],"description":"","content":"From your local machine, login to the newly created VSI by running the following command (Note: After the VSI is in running state you might still need to wait a few minutes before you can connect to it via ssh ):\nssh -J root@\u0026lt;jump-node-ip\u0026gt; root@\u0026lt;new-instance-ip\u0026gt; (Explain how to get the jump node IP)\nNow customize the software on the VSI boot volume.\n"},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/03-bursting/02-setup/","title":"Set up multi-cluster and job forwarding","tags":["burst"],"description":"","content":"et up the LSF multi-cluster and job forwarding. This tutorial assumes that the small on-premise cluster labeled with \u0026ldquo;OnPremiseCluster\u0026rdquo; uses a subnet 192.168.3.0/24. The cluster runs a master node at 192.168.3.21 (on-premise-master) and two single-CPU workers (i.e., up to two CPUs are available for jobs). The cloud cluster labeled with \u0026ldquo;HPCCluster\u0026rdquo; uses a subnet 10.248.0.32/27 and its master uses 10.248.0.37 (icgen2host-10-248-0-37) without any static workers created. Both of the configuration directories are in /opt/ibm/lsf/conf, but you can change the directory depending on your cluster configuration.\n  The following is an example of the /etc/hosts file for the cloud cluster. You need to make sure that the hostnames for the LSF masters are DNS-resolvable.\n... 10.248.0.61 icgen2host-10-248-0-61 10.248.0.62 icgen2host-10-248-0-62 10.248.0.63 icgen2host-10-248-0-63 192.168.3.21 on-premise-master # added For the on-premises /etc/hosts file, make sure to add the information about the master node in the cloud cluster:\n192.168.3.21 on-premise-master 192.168.3.22 on-premise-worker-0 192.168.3.23 on-premise-worker-1 10.248.0.37 icgen2host-10-248-0-37 #added   Both clusters need to recognize each other, so you need to modify /opt/ibm/lsf/conf/lsf.shared. This configuration file should be identical in both clusters.\n... Begin Cluster ClusterName Servers # Keyword # modified HPCCluster (icgen2host-10-248-0-37) # modified OnPremiseCluster (on-premise-master) # modified End Cluster ...   The two clusters are configured to have different lsb.queues files. In the cloud cluster, you need to append the following lines to /opt/ibm/lsf/conf/lsbatch/HPCCluster/configdir/lsb.queues to register a receive queue:\n... Begin Queue QUEUE_NAME=recv_q RCVJOBS_FROM=OnPremiseCluster PRIORITY=30 NICE=20 RC_HOSTS=all End Queue The on-premises cluster is configured to have a send queue at /opt/ibm/lsf/conf/lsbatch/OnPremiseCluster/configdir/lsb.queues:\n... Begin Queue QUEUE_NAME=send_q SNDJOBS_TO=recv_q@allclusters PRIORITY=30 NICE=20 End Queue   Restart both clusters by running the following command:\n$ lsfrestart   Check if two clusters are connected\n$ lsclusters -w CLUSTER_NAME STATUS MASTER_HOST ADMIN HOSTS SERVERS OnPremiseCluster ok on-premise-master lsfadmin 3 3 HPCCluster ok icgen2host-10-248-0-37 lsfadmin 1 1   "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/02-setup-lsf-cluster/","title":"Standing up an LSF Cluster","tags":["HPC","Introduction","LSF","Optional"],"description":"","content":"IBM Spectrum LSF is part of the HPC Clustering Services on IBM Cloud to enable users to stand up a HPC environment within minutes. LSF is an advanced workload management solution for demanding HPC environments. HPC cluster deployment starts with the definition of the desired cluster configuration; for example, count of management nodes, worker nodes, amount of storage and so on. Open-source technologies like Terraform and Ansible have been used to automate the provisioning and configuration of all associated IBM Cloud VPC resources and LSF.\nIn this section, you are using the IBM Spectrum LSF catalog tile to create a HPC cluster in IBM Cloud and run a LSF job as you would on-premises. This includes the following steps:\n Prepare your SSH key and IBM Cloud API keys. Configure cluster parameters from your IBM Cloud web console. Create your first cluster. Submit a sample job and check what is happening in the background. Delete the cluster.  "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/04-custom-image/03-create-image-from-customized-volume/","title":"Create Image From Customized Volume","tags":[],"description":"","content":"Now, we go back to the Cloud Shell for creating the new custom image.\n Stop the VSI  ibmcloud is instance-stop ${CUSTOM_IMG_VSI_NAME} Submit the request to create the custom image  # Store Volume ID in a environment variable SOURCE_VOLUME_ID=$(ibmcloud is instance ${CUSTOM_IMG_VSI_NAME} --output JSON | jq -r .volume_attachments[0].volume.id) # Submit request to create image from the volume ibmcloud is image-create ${CUSTOM_IMG_NAME} --source-volume ${SOURCE_VOLUME_ID} This step can take some time depending on the image size. You can check webpage(where to check? take screenshot) for the status of the image creation.\nWhen the image is created, we can delete the VSI that is no longer needed  ibmcloud is instance-delete ${CUSTOM_IMG_VSI_NAME} "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/03-bursting/03-tryout/","title":"Try cloud bursting","tags":["burst"],"description":"","content":"Now you can forward jobs from on-premises to the cloud. This tutorial tries to request three CPUs by starting three concurrent sleep 10s although the example on-premise cluster has only two CPUs. At your on-premises cluster, you can test the following command:\n$ for i in `seq 1 3`; do bsub -q send_q sleep 10s; done The cloud cluster needs a few minutes to allocate a dynamic host. After a while, you can check if cloud bursting works at on-premise-master.\n$ bjobs -aw JOBID USER STAT QUEUE FROM_HOST EXEC_HOST JOB_NAME SUBMIT_TIME 1 lsfadmin DONE send_q on-premise-master on-premise-worker-0 sleep 10s Oct 7 01:02 2 lsfadmin DONE send_q on-premise-master on-premise-worker-1 sleep 10s Oct 7 01:02 3 lsfadmin DONE send_q on-premise-master icgen2host-10-248-0-38@HPCCluster sleep 10s Oct 7 01:02 You can also see that the job comes to a node at HPCCluster.\n$ bjobs -aw JOBID USER STAT QUEUE FROM_HOST EXEC_HOST JOB_NAME SUBMIT_TIME 101 lsfadmin DONE recv_q on-premise-master@OnPremiseCluster:3 icgen2host-10-248-0-38 sleep 10s Oct 7 01:02 You can see icgen2host-10-248-0-38 has been created. If you do not start any jobs for a while, the host is automatically released (marked as closed_RC).\n$ bhosts -aw HOST_NAME STATUS JL/U MAX NJOBS RUN SSUSP USUSP RSV icgen2host-10-248-0-37 closed_Full - 0 0 0 0 0 0 icgen2host-10-248-0-38 closed_RC - 1 0 0 0 0 0 "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/03-bursting/","title":"Cloud Bursting","tags":["HPC","Introduction","LSF","Optional"],"description":"","content":"IBM Cloud offers virtual private clouds (VPCs), which enable elastic provisioning of x86 virtual servers with the pay-as-you-go model. Users can optimize both the cost and performance of their target workloads by maintaining the number of virtual servers and their resource configurations. VPCs support various instance flavors of virtual machines with from 1 to 128 virtual CPUs, from 4GB to 1TB RAM, and various capacity and performance settings of storage and networking.\nIn this post, we focus on an automation package for Spectrum LSF.\nIBM Spectrum LSF is widely used to manage parallel distributed HPC workloads. The automation package deploys LSF management and compute hosts with a shared filesystem. It also configures auto-scaling of compute hosts with its resource connector for IBM Cloud. The deployed cluster runs the Red Hat Enterprise Linux 7.7 with pre-installed software packages such as OpenMPI.\n The automation package enables users to quickly create an HPC cluster on IBM Cloud. Users can import data to the shared filesystem and submit HPC jobs using the bsub command. The automation can be regarded as a quick starter kit for HPC in the cloud, but also it enables cloud bursting, a technique to deal with the temporal insufficiency of computing resources at an on-premise cluster using the elastic cloud infrastructures.\nLSF can support cloud bursting using multi-cluster setups and job forwarding. With these features, LSF can forward jobs from on-premise to a cloud cluster when the on-premise cannot satisfy requests for computing resources. On the other hand, the cloud cluster can be scaled down to save the costs for compute nodes during idle time.\nThe automation package also supports the configuration of a virtual private network (VPN) for an LSF cluster. VPN enables secure network connections between on-premise and cloud clusters as if they were directly connected. On top of the VPN connectivity, two LSF clusters can securely share cluster information and forward jobs over the internet.\nThis tutorial consists of three parts:\n Setting up VPN Setting up multi-cluster and job forwarding Trying simple workloads with cloud bursting.  "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/04-custom-image/04-create-cluster-from-custom-image/","title":"Create Cluster From Custom Image","tags":[],"description":"","content":"(To be completed when the custom image feature is fixed)\n"},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/04-custom-image/","title":"Custom Image","tags":["HPC","LSF","Optional"],"description":"","content":"In this tutorial, we will guide you through the steps to create a custom image that allows you to install custom software packages and later reuse the image for creating new LSF clusters. We assume you have followed the steps in \u0026lt;02-setup-lsf-cluster\u0026gt; to create an existing LSF cluster, and we will reuse its VPC infrastructure to create a VSI within it.\n"},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/tags/hpc/","title":"HPC","tags":[],"description":"","content":""},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/tags/lsf/","title":"LSF","tags":[],"description":"","content":""},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/tags/optional/","title":"Optional","tags":[],"description":"","content":""},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/tags/","title":"Tags","tags":[],"description":"","content":""},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/tags/burst/","title":"burst","tags":[],"description":"","content":""},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/tags/introduction/","title":"Introduction","tags":[],"description":"","content":""},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/","title":"HPC on IBM Cloud Tutorials","tags":[],"description":"","content":"Welcome to HPC on IBM Cloud Tutorials === SOME INTRODUCTION ===\nSelect a topic from the nagivation panel on the left or continue to HPC on Cloud Overview.\n"},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/conferences/","title":"Conferences Workshops","tags":[],"description":"","content":"List of workshops given at confernces    Name Description     reInvent 2021 AWS ParallelClusterAPI Introduction to the AWS ParallelCluster API and the PCluster Manager web UI to manage clusters   Supercomputing 2021 Tutorial Collection of labs created for the full-day tutorial delivered by the AWS HPC team at SC21   Supercomputing 2020 Tutorial Find the labs created for the full-day tutorial delivered by the AWS HPC team at SC20   Supercomputing 2019 Tutorial Collection of labs created for the first full-day tutorial delivered by the AWS HPC team at SC19    "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/authors/","title":"Credits","tags":[],"description":"","content":" I-hsin Chung Ming Hung Chen Constantinos Evangelinos Lixiang Luo Dale Pearson Seetharami Seelam Robert Walkup Hui-fang Wen Chih Chieh Yang  "},{"uri":"https://ibm.github.io/ibmcloud-hpc-tutorials/external/","title":"External Workshops","tags":[],"description":"","content":"List of External workshops    Name Description     Advanced Slurm on ParallelCluster This workshop shows advanced techniques with Slurm on ParallelCluster, including federation, accounting, and using the Slurm REST API with Jupyter notebooks   Setting up a client library for the Slurm REST API A Jupyter notebook walkthrough of building and using a Python client library for the Slurm REST API    "}]